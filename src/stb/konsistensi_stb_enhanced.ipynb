{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce76f74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openpyxl import load_workbook\n",
    "import yaml\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef8523c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = r'C:/Users/amnar/Desktop/gh_konsistensi/'\n",
    "yaml_path = 'ref/stb_config_all_2021.yml'\n",
    "\n",
    "# File paths\n",
    "dataset_path = base_path + 'data/stb/dsB092021STB.xlsx'\n",
    "reference_files = ['ref/oku.csv',\n",
    "                   'ref/kewarganegaraan.csv',\n",
    "                   'ref/kumpulan_etnik.csv', \n",
    "                   'ref/persekolahan.csv',\n",
    "                   'ref/pendidikan_rasmi.csv',\n",
    "                   'ref/pendidikan_rasmi_tertinggi_2022.csv', \n",
    "                   'ref/sijil_tertinggi.csv',\n",
    "                   'ref/sijil_tertinggi_2022.csv',\n",
    "                   'ref/status_code.csv', \n",
    "                   'ref/msic_code_detail_01.csv',\n",
    "                   'ref/masco_code.csv',\n",
    "                   'ref/negara_code.csv', \n",
    "                   'ref/institusi_pengajian.csv',\n",
    "                   'ref/bidang_pengajian.csv']\n",
    "\n",
    "file_paths = [dataset_path] + [base_path + file for file in reference_files]\n",
    "yaml_file = base_path + yaml_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473d4d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(*file_paths):\n",
    "    def read_csv(file_path):\n",
    "        return pd.read_csv(file_path, encoding='unicode_escape', low_memory=False)\n",
    "\n",
    "    def read_excel(file_path):\n",
    "        workbook = load_workbook(filename=file_path)\n",
    "        sheet_name = workbook.sheetnames[0]  # Get the name of the first sheet\n",
    "        worksheet = workbook[sheet_name]\n",
    "        data = list(worksheet.values)\n",
    "        return pd.DataFrame(data[1:], columns=data[0])\n",
    "\n",
    "    file_readers = {\n",
    "        'csv': read_csv,\n",
    "        'xlsx': read_excel,\n",
    "        'xls': read_excel\n",
    "    }\n",
    "    \n",
    "    predefined_names = {\n",
    "        'dsB092021STB.xlsx': 'df',\n",
    "        'oku.csv': 'df_oku',\n",
    "        'kewarganegaraan.csv': 'df_kw',\n",
    "        'kumpulan_etnik.csv': 'df_ket',\n",
    "        'persekolahan.csv': 'df_persk',\n",
    "        'pendidikan_rasmi.csv': 'df_pend',\n",
    "        'pendidikan_rasmi_tertinggi_2022.csv': 'df_pend_22',\n",
    "        'sijil_tertinggi.csv': 'df_sijil',\n",
    "        'sijil_tertinggi_2022.csv': 'df_sijil_22',\n",
    "        'status_code.csv': 'df_status',\n",
    "        'msic_code_detail_01.csv': 'df_msic',\n",
    "        'masco_code.csv': 'df_masco',\n",
    "        'negara_code.csv': 'df_ngra',\n",
    "        'institusi_pengajian.csv': 'df_ip',\n",
    "        'bidang_pengajian.csv': 'df_fs'\n",
    "    }\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        filename = file_path.split('/')[-1]\n",
    "        df_name = predefined_names[filename]\n",
    "        file_format = filename.split('.')[-1]\n",
    "        \n",
    "        # Declare the dataframe name as global\n",
    "        globals()[df_name] = file_readers[file_format](file_path)\n",
    "\n",
    "# Call the function\n",
    "read_files(*file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3ec127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the YAML file\n",
    "with open(yaml_file, 'r') as file:\n",
    "    config = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "# Extract conditions from the YAML data\n",
    "persekolahan = config['persekolahan']\n",
    "\n",
    "# Create an empty dictionary to store the merged values\n",
    "merged_conditions = {}\n",
    "\n",
    "# Loop over the persekolahan dictionaries and merge the values\n",
    "for key, conditions in persekolahan.items():\n",
    "    merged_conditions[key] = {}\n",
    "    for condition_key, condition_value in conditions.items():\n",
    "        if condition_key == 'U':\n",
    "            if isinstance(condition_value, str) and condition_value.isdigit():\n",
    "                condition_value = int(condition_value)\n",
    "        merged_conditions[key][condition_key] = condition_value\n",
    "        \n",
    "for key in persekolahan:\n",
    "    persekolahan[key]['U'] = list(eval(persekolahan[key]['U']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549c0095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the dataset name from the file path\n",
    "dataset_name = os.path.basename(dataset_path)\n",
    "\n",
    "# Extract the first three characters\n",
    "doc_type = dataset_name[:3]\n",
    "\n",
    "# Extract the characters at index 3 and 4\n",
    "quarter_ref = dataset_name[3:5]\n",
    "\n",
    "# Extract the characters from index 6 to 9\n",
    "year_ref = int(dataset_name[5:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd0748f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the string \"None\" (and its variants with potential spaces) with NaN for the entire DataFrame\n",
    "for col in df.columns:\n",
    "    df[col] = df[col].apply(lambda x: np.nan if str(x).strip() == 'None' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e56d22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def format_columns(data, columns_to_format):\n",
    "#     # Standardize columns with 6 digits and leading zeros\n",
    "#     for column in columns_to_format['leading_zeros_6_digits']:\n",
    "#         if column in data.columns:\n",
    "#             data[column] = data[column].apply(lambda x: str(int(x)).zfill(6) if not pd.isnull(x) else x)\n",
    "#             print(f\"Formatted {column} to have 6 digits with leading zeros.\")\n",
    "#         else:\n",
    "#             print(f\"Column '{column}' not found in the DataFrame.\")\n",
    "\n",
    "#     # Standardize columns with 5 digits and leading zeros\n",
    "#     for column in columns_to_format['leading_zeros_5_digits']:\n",
    "#         if column in data.columns:\n",
    "#             data[column] = data[column].apply(lambda x: str(int(x)).zfill(5) if not pd.isnull(x) else x)\n",
    "#             print(f\"Formatted {column} to have 5 digits with leading zeros.\")\n",
    "#         else:\n",
    "#             print(f\"Column '{column}' not found in the DataFrame.\")\n",
    "            \n",
    "#     # Standardize columns with 5 digits and leading zeros\n",
    "#     for column in columns_to_format['leading_zeros_4_digits']:\n",
    "#         if column in data.columns:\n",
    "#             data[column] = data[column].apply(lambda x: str(int(x)).zfill(4) if not pd.isnull(x) else x)\n",
    "#             print(f\"Formatted {column} to have 4 digits with leading zeros.\")\n",
    "#         else:\n",
    "#             print(f\"Column '{column}' not found in the DataFrame.\")\n",
    "            \n",
    "#     # Standardize columns with 3 digits and leading zeros\n",
    "#     for column in columns_to_format['leading_zeros_3_digits']:\n",
    "#         if column in data.columns:\n",
    "#             data[column] = data[column].apply(lambda x: str(int(x)).zfill(3) if not pd.isnull(x) else x)\n",
    "#             print(f\"Formatted {column} to have 3 digits with leading zeros.\")\n",
    "#         else:\n",
    "#             print(f\"Column '{column}' not found in the DataFrame.\")\n",
    "            \n",
    "#     # Standardize columns with 2 digits and leading zeros\n",
    "#     for column in columns_to_format['leading_zeros_2_digits']:\n",
    "#         if column in data.columns:\n",
    "#             data[column] = data[column].apply(lambda x: str(int(x)).zfill(2) if not pd.isnull(x) else x)\n",
    "#             print(f\"Formatted {column} to have 2 digits with leading zeros.\")\n",
    "#         else:\n",
    "#             print(f\"Column '{column}' not found in the DataFrame.\")\n",
    "\n",
    "#     # Convert columns to specific formats\n",
    "#     for column, data_type in columns_to_format['data_types'].items():\n",
    "#         if column in data.columns:\n",
    "#             if data_type == int:\n",
    "#                 data[column] = data[column].apply(lambda x: int(x) if pd.notnull(x) and str(x).isdigit() else x)\n",
    "#                 print(f\"Converted {column} to {data_type}.\")\n",
    "#             else:\n",
    "#                 data[column] = data[column].astype(data_type)\n",
    "#                 print(f\"Converted {column} to {data_type}.\")\n",
    "#         else:\n",
    "#             print(f\"Column '{column}' not found in the DataFrame.\")\n",
    "\n",
    "#     # Standardize columns with 7 digits and two decimal places (string)\n",
    "#     for column in columns_to_format['standardize_7_digits']:\n",
    "#         if column in data.columns:\n",
    "#             data[column] = data[column].apply(lambda x: f\"{float(x):08.2f}\" if pd.notnull(x) and (isinstance(x, float) or (isinstance(x, str) and x.replace('.', '', 1).isdigit())) else x)\n",
    "#             print(f\"Standardized {column} to have 7 digits and two decimal places.\")\n",
    "#         else:\n",
    "#             print(f\"Column '{column}' not found in the DataFrame.\")\n",
    "\n",
    "#     # Return the modified DataFrame\n",
    "#     return data\n",
    "\n",
    "\n",
    "\n",
    "# columns_to_format_01 = {\n",
    "#     'leading_zeros_6_digits': [],\n",
    "#     'leading_zeros_5_digits': ['S19', 'S18'],\n",
    "#     'leading_zeros_4_digits': ['KW', 'NGRA', 'FS', 'MASCO_4D'],\n",
    "#     'leading_zeros_3_digits': ['KW', 'NGRA', 'FS', 'MASCO_3D', 'NO KEL'],\n",
    "#     'leading_zeros_2_digits': ['PKIS', 'BK', 'NGRI', 'PT', 'SJ', 'Grp_SJ', 'MASCO_2D'],\n",
    "#     'standardize_7_digits': [],\n",
    "#     'data_types': {\n",
    "#         'P': int,\n",
    "#         'U': int,\n",
    "#         'PT': str,\n",
    "#         'SJ': str,\n",
    "#         'HMIS': int,\n",
    "#         'HMWA': int,\n",
    "#         'MSIC_1D': str,\n",
    "#         'RIN': int,\n",
    "#         'B': int,\n",
    "#         'NG': int,\n",
    "#         'DP': int,\n",
    "#         'ST': int,\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# columns_to_format_02 = {\n",
    "#     'leading_zeros_6_digits': ['KOD_MASCO'],\n",
    "#     'leading_zeros_5_digits': [],\n",
    "#     'leading_zeros_4_digits': [],\n",
    "#     'leading_zeros_3_digits': [],\n",
    "#     'leading_zeros_2_digits': [],\n",
    "#     'standardize_7_digits': [],\n",
    "#     'data_types': {\n",
    "        \n",
    "#     }\n",
    "# }\n",
    "\n",
    "# columns_to_format_03 = {\n",
    "#     'leading_zeros_6_digits': [],\n",
    "#     'leading_zeros_5_digits': ['KOD_MSIC'],\n",
    "#     'leading_zeros_4_digits': [],\n",
    "#     'leading_zeros_3_digits': [],\n",
    "#     'leading_zeros_2_digits': [],\n",
    "#     'standardize_7_digits': [],\n",
    "#     'data_types': {\n",
    "        \n",
    "#     }\n",
    "# }\n",
    "\n",
    "# columns_to_format_04 = {\n",
    "#     'leading_zeros_6_digits': ['Kod'],\n",
    "#     'leading_zeros_5_digits': [],\n",
    "#     'leading_zeros_4_digits': [],\n",
    "#     'leading_zeros_3_digits': [],\n",
    "#     'leading_zeros_2_digits': [],\n",
    "#     'standardize_7_digits': [],\n",
    "#     'data_types': {\n",
    "        \n",
    "#     }\n",
    "# }\n",
    "\n",
    "# formatted_data_01 = format_columns(df, columns_to_format_01)\n",
    "# formatted_data_02 = format_columns(df_masco, columns_to_format_02)\n",
    "# formatted_data_03 = format_columns(df_msic, columns_to_format_03)\n",
    "# formatted_data_04 = format_columns(df_status, columns_to_format_04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78976aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_list = df_kw[\"Kod\"].astype(str).apply(lambda x: x.zfill(3)).tolist()\n",
    "ket_list = list(map(int, df_ket.iloc[:97][\"Kod\"].values))\n",
    "status_list = df_status[\"Kod\"].values.tolist()\n",
    "masco_list = df_masco[\"KOD_MASCO\"].values.tolist()\n",
    "msic_list = df_msic[\"KOD_MSIC\"].values.tolist()\n",
    "pkis_list = [str(i).zfill(2) for i in range(1, 13)]\n",
    "no_kel_list = [str(i).zfill(3) for i in range(1, 1000)]\n",
    "ngri_list = df_ngra[\"KOD\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf4e7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Semakan Julat JR4 ###\n",
    "\n",
    "kel_list = list(range(1000))\n",
    "b_list = list(range(13))\n",
    "ng_list = list(range(17))\n",
    "dp_list = list(range(32))\n",
    "db_list = list(range(170))\n",
    "db2_list = set(str(i).zfill(5) for i in range(1, 100000))\n",
    "st_list = list(range(10))\n",
    "notk_list = list(range(1000))\n",
    "noir_list = list(range(100))\n",
    "t_list = list(range(100))\n",
    "pkis_list = [str(i).zfill(2) for i in range(1, 13)]\n",
    "hmis_list = [str(i).zfill(2) for i in range(1, 100)]\n",
    "bk_list = [str(i).zfill(2) for i in range(1, 13)]\n",
    "tk_list = [str(i).zfill(4) for i in range(1900, 3000)]\n",
    "u_list = list(range(201))\n",
    "tp_list = list(range(6))\n",
    "ngri_list = [str(i).zfill(2) for i in range(1, 17)] + ['98']\n",
    "ngra_list = df_ngra[\"KOD\"].astype(str).apply(lambda x: x.zfill(3)).tolist()\n",
    "oku_list = df_oku[\"Kod\"].astype(str).apply(lambda x: x.zfill(2)).tolist()\n",
    "pt_22_list = df_pend_22[\"KOD\"].astype(str).apply(lambda x: x.zfill(3)).tolist()\n",
    "sj_22_list = df_sijil_22[\"KOD\"].astype(str).apply(lambda x: x.zfill(3)).tolist()\n",
    "ip_list = df_ip[\"Kod\"].tolist()\n",
    "fs_list = df_fs[\"Kod\"].astype(str).apply(lambda x: x.zfill(4)).tolist()\n",
    "hmwa_list = list(range(100))\n",
    "\n",
    "def validate_all_julats(row):\n",
    "    row['JULAT_001'] = 1 if row['NOKEL'] in no_kel_list else 0\n",
    "    row['JULAT_002'] = 1 if row['B'] in b_list else 0\n",
    "    row['JULAT_003'] = 1 if row['NG'] in ng_list else 0\n",
    "    row['JULAT_004'] = 1 if row['DP'] in dp_list else 0\n",
    "    row['JULAT_005'] = 1 if row['DB'] in db_list else 0\n",
    "    row['JULAT_006'] = 1 if row['BP'] in db_list else 0\n",
    "    row['JULAT_007'] = 1 if row['BP2'] in db2_list else 0\n",
    "    row['JULAT_008'] = 1 if row['ST'] in st_list else 0\n",
    "    row['JULAT_009'] = 1 if row['NOTK'] in notk_list else 0\n",
    "    row['JULAT_010'] = 1 if row['NOIR'] in noir_list else 0\n",
    "    row['JULAT_011'] = 1 if row['JR'] == 4 else 0\n",
    "    row['JULAT_012'] = 1 if isinstance(row['NAMA'], str) and len(row['NAMA']) <= 50 else 0\n",
    "    row['JULAT_013'] = 1 if isinstance(row['NOIC'], str) and len(row['NOIC']) <= 12 or pd.isna(row['NOIC']) else 0\n",
    "    row['JULAT_014'] = 1 if row['PKIS'] in pkis_list else 0\n",
    "    row['JULAT_015'] = 1 if row['HMIS'] in hmis_list else 0\n",
    "    row['JULAT_016'] = 1 if row['J'] in [1, 2] else 0\n",
    "    row['JULAT_017'] = 1 if row['BK'] in bk_list else 0\n",
    "    row['JULAT_018'] = 1 if row['TK'] in tk_list else 0\n",
    "    row['JULAT_019'] = 1 if row['U'] in u_list else 0\n",
    "    row['JULAT_020'] = 1 if row['KET'] in ket_list else 0\n",
    "    row['JULAT_021'] = 1 if row['KW'] in kw_list else 0\n",
    "    row['JULAT_022'] = 1 if row['TP'] in tp_list else 0\n",
    "    row['JULAT_023'] = 1 if row['NGRI'] in pkis_list else 0\n",
    "    row['JULAT_024'] = 1 if row['NGRA'] in ngra_list else 0\n",
    "    row['JULAT_025'] = 1 if row['OKU'] in oku_list else 0\n",
    "    row['JULAT_026'] = 1 if row['P'] in [1, 2, 3, 4] else 0\n",
    "    row['JULAT_027'] = 1 if row['PT'] in pt_22_list else 0\n",
    "    row['JULAT_028'] = 1 if row['SJ'] in sj_22_list else 0\n",
    "    row['JULAT_029'] = 1 if row['IP'] in ip_list or pd.isna(row['IP']) else 0\n",
    "    row['JULAT_030'] = 1 if row['FS'] in fs_list else 0\n",
    "    row['JULAT_031'] = 1 if row['HMWA'] in hmwa_list else 0\n",
    "    \n",
    "    return row\n",
    "\n",
    "df = df.apply(validate_all_julats, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cedacc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# Konsistensi 1(a)\n",
    "# -----------------------------------\n",
    "\n",
    "# This function checks a condition based on the 'TK' and 'U' columns of the dataframe.\n",
    "# If the difference between a reference year and the sum of these columns is greater than 3, \n",
    "# it sets the value of 'KONSISTENSI_01a' to 0. Otherwise, it sets the value to 1.\n",
    "def validate_condition_01(df, year_ref):\n",
    "    def compute(row):\n",
    "        if (year_ref - int(row['TK']) - int(row['U'])) > 3:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "    \n",
    "    df['KONSISTENSI_01'] = df.apply(lambda row: compute(row) if row['TK'].isdigit() and row['U'].isdigit() else 0, axis=1).astype(int)\n",
    "\n",
    "validate_condition_01(df, year_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700f26b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# Konsistensi 1(b)\n",
    "# -----------------------------------\n",
    "\n",
    "# This function checks two conditions:\n",
    "# 1. If 'U' is less than or equal to 15, it sets 'KONSISTENSI_02' to 1.\n",
    "# 2. If 'U' is greater than 15 and 'HMWA' is not null, it sets 'KONSISTENSI_02' to 1.\n",
    "def validate_condition_02(df):\n",
    "    df['KONSISTENSI_02'] = 0\n",
    "    df.loc[df['U'].astype(int) <= 15, 'KONSISTENSI_02'] = 1\n",
    "    combined_condition = (df['U'].astype(int) > 15) & ~df['HMWA'].isnull()\n",
    "    df.loc[combined_condition, 'KONSISTENSI_02'] = 1\n",
    "\n",
    "validate_condition_02(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e110d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# Konsistensi 1(c)\n",
    "# -----------------------------------\n",
    "\n",
    "# This function checks the values of 'NGRI' for rows where 'NGRA' is 458.\n",
    "# If 'NGRI' is not in the provided list of allowed values, it sets 'KONSISTENSI_03' to 0.\n",
    "def validate_condition_03(df, values):\n",
    "    df['KONSISTENSI_03'] = 1\n",
    "    df_ngra = df[df['NGRA'] == '458']\n",
    "    mask = ~df_ngra['NGRI'].isin(values)\n",
    "    df.loc[df_ngra[mask].index, 'KONSISTENSI_03'] = 0\n",
    "\n",
    "ngri_lst = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14', '15', '16']\n",
    "\n",
    "validate_condition_03(df, ngri_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869a33f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# Konsistensi 1(d)\n",
    "# -----------------------------------\n",
    "\n",
    "# This function checks if values in the 'OKU' column are present in the provided list of OKU values.\n",
    "# If a match is found, it sets 'KONSISTENSI_04' to 1, otherwise 0.\n",
    "def validate_condition_04(df, oku_list):\n",
    "    df['KONSISTENSI_04'] = df['OKU'].astype(int).isin(oku_list).astype(int)\n",
    "\n",
    "oku_list = list(map(int, df_oku[\"Kod\"].values))\n",
    "validate_condition_04(df, oku_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6407b76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# Konsistensi 1(e)\n",
    "# -----------------------------------\n",
    "\n",
    "# This function checks if values in the 'KW' column are present in the provided list.\n",
    "# If a match is found, it sets 'KONSISTENSI_05a' to 1, otherwise 0.\n",
    "def validate_condition_05a(df, kw):\n",
    "    df['KONSISTENSI_05a'] = df['KW'].astype(str).isin(kw).astype(int)\n",
    "\n",
    "# This function checks if values in the 'KET' column are present in the provided list.\n",
    "# If a match is found, it sets 'KONSISTENSI_05b' to 1, otherwise 0.\n",
    "def validate_condition_05b(df, ket):\n",
    "    df['KONSISTENSI_05b'] = df['KET'].astype(int).isin(ket).astype(int)\n",
    "\n",
    "validate_condition_05a(df, kw_list)\n",
    "validate_condition_05b(df, ket_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee764b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# Konsistensi 1(f)\n",
    "# -----------------------------------\n",
    "\n",
    "# This function checks if any row of the dataframe meets any of the conditions specified in merged_conditions.\n",
    "# If a row meets any of the conditions, it sets 'KONSISTENSI_06' for that row to 1.\n",
    "def validate_condition_06(df, merged_conditions):\n",
    "    df['KONSISTENSI_06'] = 0\n",
    "    for index, row in df.iterrows():\n",
    "        for condition_key, condition_values in merged_conditions.items():\n",
    "            match = True\n",
    "            for col, val in condition_values.items():\n",
    "                if isinstance(val, list):\n",
    "                    if row[col] not in val:\n",
    "                        match = False\n",
    "                        break\n",
    "                else:\n",
    "                    if row[col] != val:\n",
    "                        match = False\n",
    "                        break\n",
    "            if match:\n",
    "                df.loc[index, 'KONSISTENSI_06'] = 1\n",
    "                break\n",
    "\n",
    "validate_condition_06(df, persekolahan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6774ff69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# Konsistensi 1(g)\n",
    "# -----------------------------------\n",
    "\n",
    "# This function checks two conditions:\n",
    "# 1. If 'U' is greater than or equal to 18, it sets 'KONSISTENSI_08' to 1.\n",
    "# 2. If 'U' is less than 18 and 'TP' is in the provided list, it sets 'KONSISTENSI_08' to 1.\n",
    "def validate_condition_08(df, tp_lst):\n",
    "    df['KONSISTENSI_08'] = 0\n",
    "    df.loc[df['U'].astype(int) >= 18, 'KONSISTENSI_08'] = 1\n",
    "    combined_condition = (df['U'].astype(int) < 18) & df['TP'].isin(tp_lst)\n",
    "    df.loc[combined_condition, 'KONSISTENSI_08'] = 1\n",
    "\n",
    "validate_condition_08(df, tp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44af86ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# Konsistensi 1(h)\n",
    "# -----------------------------------\n",
    "\n",
    "# This function checks two conditions:\n",
    "# 1. If 'U' is greater than 15 and 'HMWA' is not null.\n",
    "# 2. If the first condition is met, it further checks if 'HMWA' and 'HMIS\n",
    "\n",
    "def validate_condition_09(df):\n",
    "    df['KONSISTENSI_09'] = 0\n",
    "    condition_1 = (df['U'].astype(int) > 15) & ~df['HMWA'].isnull()\n",
    "    condition_2 = df['HMWA'] == df['HMIS']\n",
    "    df.loc[condition_1 & condition_2, 'KONSISTENSI_09'] = 1\n",
    "\n",
    "validate_condition_09(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d25bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# Konsistensi T-C1a\n",
    "# -----------------------------------\n",
    "\n",
    "def validate_condition_TC1a(data):\n",
    "    \"\"\"\n",
    "    Validate consistency for 'MSIC_1D', 'S19', and 'S20' in the DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "        data (pd.DataFrame): DataFrame containing 'MSIC_1D', 'S19', and 'S20'\n",
    "                             columns to be validated.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with an additional column 'KONSISTENSI_TC1a',\n",
    "                      indicating validation results.\n",
    "    \"\"\"\n",
    "    # Initialize 'KONSISTENSI_TC1a' with 1 for all rows.\n",
    "    data['KONSISTENSI_TC1a'] = 1\n",
    "    \n",
    "    # Identify rows that need further validation based on 'MSIC_1D' and 'S19'.\n",
    "    filtered_rows = (\n",
    "        (data['MSIC_1D'] == 'O') &\n",
    "        data['S19'].between('84111', '84300')\n",
    "    )\n",
    "    \n",
    "    # For the filtered rows, if 'S20' is not '2', mark them as fail (0).\n",
    "    data.loc[filtered_rows & (data['S20'] != '2'), 'KONSISTENSI_TC1a'] = 0\n",
    "    \n",
    "    return data\n",
    "\n",
    "result = validate_condition_TC1a(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febd3393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# Konsistensi T-C1b\n",
    "# -----------------------------------\n",
    "\n",
    "def validate_condition_TC1b(data):\n",
    "    \"\"\"\n",
    "    Validate consistency for 'MSIC_1D', 'S19', and 'KW' in the DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "        data (pd.DataFrame): DataFrame containing 'MSIC_1D', 'S19', and 'KW'\n",
    "                             columns to be validated.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with an additional column 'KONSISTENSI_TC1b',\n",
    "                      indicating validation results.\n",
    "    \"\"\"\n",
    "    # Initialize 'KONSISTENSI_TC1b' with 1 for all rows.\n",
    "    data['KONSISTENSI_TC1b'] = 1\n",
    "    \n",
    "    # Identify rows that need further validation based on 'MSIC_1D' and 'S19'.\n",
    "    filtered_data_TC1b = data[\n",
    "        (data['MSIC_1D'] == 'O') &\n",
    "        data['S19'].between('84111', '84300')\n",
    "    ].copy()\n",
    "    \n",
    "    # For the filtered rows, if 'KW' is 458, mark them as fail (0).\n",
    "    data.loc[data.index.isin(filtered_data_TC1b.index) & (data['KW'] == 458), 'KONSISTENSI_TC1b'] = 0\n",
    "    \n",
    "    return data\n",
    "\n",
    "result = validate_condition_TC1b(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1d0209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# Konsistensi T-C2\n",
    "# -----------------------------------\n",
    "\n",
    "def validate_condition_TC2(data):\n",
    "    \"\"\"\n",
    "    Validate consistency based on conditions for 'MSIC_1D' and 'S20'.\n",
    "    \n",
    "    Parameters:\n",
    "        data (pd.DataFrame): DataFrame containing 'MSIC_1D' and 'S20'\n",
    "                             columns to be validated.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with an additional column 'KONSISTENSI_TC2',\n",
    "                      indicating validation results.\n",
    "    \"\"\"\n",
    "    # Initialize 'KONSISTENSI_TC2' with 1 for all rows (assume pass initially).\n",
    "    data['KONSISTENSI_TC2'] = 1\n",
    "    \n",
    "    # Identify rows that need further validation based on 'MSIC_1D'.\n",
    "    filtered_data_TC2 = data[(data['MSIC_1D'] == 'T')].copy()\n",
    "    \n",
    "    # For the filtered rows, if 'S20' is not 3 and not NaN, mark them as fail (0).\n",
    "    data.loc[data.index.isin(filtered_data_TC2.index) & (data['S20'].astype(str) != 3) & (data['S20'].notna()), 'KONSISTENSI_TC2'] = 0      \n",
    "    \n",
    "    return data\n",
    "    \n",
    "result = validate_condition_TC2(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed21ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# Konsistensi T-C3\n",
    "# -----------------------------------\n",
    "\n",
    "def validate_condition_TC3(data):\n",
    "    \"\"\"\n",
    "    Validate consistency based on conditions for 'MSIC_1D' and 'S20'.\n",
    "    \n",
    "    Parameters:\n",
    "        data (pd.DataFrame): DataFrame containing 'MSIC_1D' and 'S20'\n",
    "                             columns to be validated.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with an additional column 'KONSISTENSI_TC3',\n",
    "                      indicating validation results.\n",
    "    \"\"\"\n",
    "    # Initialize 'KONSISTENSI_TC3' with 1 for all rows (assume pass initially).\n",
    "    data['KONSISTENSI_TC3'] = 1\n",
    "    \n",
    "    # Identify rows that need further validation based on 'MSIC_1D'.\n",
    "    filtered_data_TC3 = data[(data['MSIC_1D'] == 'P')].copy()\n",
    "    \n",
    "    # For the filtered rows, if 'S20' is not 2 and not NaN, mark them as fail (0).\n",
    "    data.loc[data.index.isin(filtered_data_TC3.index) & (data['S20'].astype(str) != '2') & (data['S20'].notna()), 'KONSISTENSI_TC3'] = 0             \n",
    "    \n",
    "    return data\n",
    "\n",
    "result = validate_condition_TC3(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d687c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae5ed93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# Konsistensi T-C4\n",
    "# -----------------------------------\n",
    "\n",
    "def validate_condition_TC4(data):\n",
    "    \"\"\"\n",
    "    Validate consistency based on conditions for 'MSIC_1D', 'MASCO_1D', and 'S20'.\n",
    "    \n",
    "    Parameters:\n",
    "        data (pd.DataFrame): DataFrame containing 'MSIC_1D', 'MASCO_1D', and 'S20' \n",
    "                             columns to be validated.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with an additional column 'KONSISTENSI_TC4',\n",
    "                      indicating validation results.\n",
    "    \"\"\"\n",
    "    # Initialize 'KONSISTENSI_TC4' with 1 for all rows (assume pass initially).\n",
    "    data['KONSISTENSI_TC4'] = 1\n",
    "    \n",
    "    # Identify rows that need further validation based on 'MSIC_1D' and 'MASCO_1D'.\n",
    "    filtered_data_TC4 = data[(data['MSIC_1D'] == 'A') & (data['MASCO_1D'] == 9)].copy()\n",
    "    \n",
    "    # For the filtered rows, if 'S20' is not 3, 4, or 5 and not NaN, mark them as fail (0).\n",
    "    data.loc[data.index.isin(filtered_data_TC4.index) & (~data['S20'].isin(['3', '4', '5'])) & (data['S20'].notna()), 'KONSISTENSI_TC4'] = 0       \n",
    "    \n",
    "    return data\n",
    "\n",
    "result = validate_condition_TC4(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e4be85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# Konsistensi T-C5a\n",
    "# -----------------------------------\n",
    "\n",
    "def validate_condition_TC5a(data):\n",
    "    \"\"\"\n",
    "    Validate consistency based on conditions for 'S18' and 'SJ'.\n",
    "    \n",
    "    Parameters:\n",
    "        data (pd.DataFrame): DataFrame containing 'S18' and 'SJ' columns to be validated.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with an additional column 'KONSISTENSI_TC5a',\n",
    "                      indicating validation results.\n",
    "    \"\"\"\n",
    "    # Ensure 'SJ' is treated as a numeric column, converting invalid entries to NaN.\n",
    "    data['SJ'] = pd.to_numeric(data['SJ'], errors='coerce')\n",
    "    \n",
    "    # Initialize 'KONSISTENSI_TC5a' with 1 for all rows (assume pass initially).\n",
    "    data['KONSISTENSI_TC5a'] = 1\n",
    "    \n",
    "    # Identify rows that meet the first condition for further validation.\n",
    "    filtered_data_TC5a = data[(data['S18'] >= '111101') & (data['S18'] <= '291918')].copy()\n",
    "    \n",
    "    # For the filtered rows, if 'SJ' is NOT between 20 and 242 and is not NaN, \n",
    "    # mark them as fail (0).\n",
    "    data.loc[data.index.isin(filtered_data_TC5a.index) & (~data['SJ'].between(20, 242)) & (data['SJ'].notna()), 'KONSISTENSI_TC5a'] = 0       \n",
    "    \n",
    "    return data\n",
    "\n",
    "result = validate_condition_TC5a(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1876c2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# Konsistensi T-C5b\n",
    "# -----------------------------------\n",
    "\n",
    "def validate_condition_TC5b(data):\n",
    "    \"\"\"\n",
    "    Validate consistency based on conditions for 'S18' and 'S20'.\n",
    "    \n",
    "    Conditions:\n",
    "    1. Rows where 'S18' is between '111101' and '291918' are considered for validation.\n",
    "    2. Initially, all entries are assumed to pass (KONSISTENSI_TC5b = 1).\n",
    "    3. Entries that fulfill condition 1 but have 'S20' equal to '4' are marked as fail (KONSISTENSI_TC5b = 0).\n",
    "    \n",
    "    Parameters:\n",
    "        data (pd.DataFrame): DataFrame containing 'S18' and 'S20' columns to be validated.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with an additional column 'KONSISTENSI_TC5b',\n",
    "                      indicating validation results.\n",
    "    \"\"\"\n",
    "    # Ensure 'S18' and 'S20' are treated as strings.\n",
    "    data[['S18', 'S20']] = data[['S18', 'S20']].astype(str)\n",
    "    \n",
    "    # Initialize 'KONSISTENSI_TC5b' with 1 for all rows (assume pass initially).\n",
    "    data['KONSISTENSI_TC5b'] = 1\n",
    "    \n",
    "    # Identify rows that meet the first condition for further validation.\n",
    "    filtered_data_TC5b = data[data['S18'].between('111101', '291918')].copy()\n",
    "    \n",
    "    # For the filtered rows, if 'S20' is '4', mark them as fail (0).\n",
    "    data.loc[data.index.isin(filtered_data_TC5b.index) & (data['S20'] == '4'), 'KONSISTENSI_TC5b'] = 0\n",
    "    \n",
    "    return data\n",
    "\n",
    "result = validate_condition_TC5b(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4091ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# Konsistensi T-C6\n",
    "# -----------------------------------\n",
    "\n",
    "def validate_condition_TC6(data):\n",
    "    \"\"\"\n",
    "    Validate consistency based on conditions for 'STATUS' and 'S20'.\n",
    "    \n",
    "    Conditions:\n",
    "    1. Rows where 'S20' is equal to 2 are considered for validation.\n",
    "    2. Initially, all entries are assumed to pass (KONSISTENSI_TC6 = 1).\n",
    "    3. Entries that fulfill condition 1 but have 'STATUS' not equal to 'GOV' and not NaN are marked as fail (KONSISTENSI_TC6 = 0).\n",
    "    \n",
    "    Parameters:\n",
    "        data (pd.DataFrame): DataFrame containing 'STATUS' and 'S20' columns to be validated.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with an additional column 'KONSISTENSI_TC6',\n",
    "                      indicating validation results.\n",
    "    \"\"\"\n",
    "    # Ensure 'S20' is treated as a string. 'STATUS' will also be treated as string but NaN will be kept as is.\n",
    "    data['S20'] = data['S20'].astype(str)\n",
    "    \n",
    "    # Initialize 'KONSISTENSI_TC6' with 1 for all rows (assume pass initially).\n",
    "    data['KONSISTENSI_TC6'] = 1\n",
    "    \n",
    "    # Identify rows that meet the first condition for further validation.\n",
    "    filtered_data_TC6 = data[data['S20'] == '2'].copy()\n",
    "    \n",
    "    # For the filtered rows, if 'STATUS' is not 'GOV' and not NaN, mark them as fail (0).\n",
    "    data.loc[data.index.isin(filtered_data_TC6.index) & (data['STATUS'] != 'GOV') & data['STATUS'].notna(), 'KONSISTENSI_TC6'] = 0      \n",
    "    \n",
    "    return data\n",
    "\n",
    "result = validate_condition_TC6(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c543591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# Konsistensi T-C7\n",
    "# -----------------------------------\n",
    "\n",
    "def validate_condition_TC7(data, masco_list):\n",
    "    # Initialize a new column 'KONSISTENSI_TC7' and set it as 1 (pass) for all rows initially\n",
    "    data['KONSISTENSI_TC7'] = 1\n",
    "    \n",
    "    # Filter data according to the first condition\n",
    "    filtered_data_TC7 = data[(data['S18'].isin(masco_list)) & (data['S20'] == '2')].copy()\n",
    "    \n",
    "    # Apply the second condition:\n",
    "    # For the filtered rows, if 'STATUS' is not 'GOV' and is not NaN, mark them as fail (0).\n",
    "    data.loc[data.index.isin(filtered_data_TC7.index) & (data['STATUS'] != 'GOV') & data['STATUS'].notna(), 'KONSISTENSI_TC7'] = 0    \n",
    "    \n",
    "    return data\n",
    "\n",
    "result = validate_condition_TC7(df, masco_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872f25c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# Konsistensi T-C8a\n",
    "# -----------------------------------\n",
    "\n",
    "def validate_condition_TC8a(data):\n",
    "    \"\"\"\n",
    "    Validates the condition T-C8a.\n",
    "\n",
    "    Parameters:\n",
    "        data (pd.DataFrame): The data on which to perform the validation.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The data with an additional column indicating the \n",
    "                      results of the validation.\n",
    "                      \n",
    "    Explanation:\n",
    "        - First Condition: \n",
    "            The data is filtered based on the condition (data['S20'] == 2). \n",
    "            Rows that do not satisfy this condition are considered as pass \n",
    "            (KONSISTENSI_TC8a = 1).\n",
    "            \n",
    "        - Second Condition: \n",
    "            From the filtered data, the 'S19' should be between '84111' and '84300' \n",
    "            (inclusive). If 'S19' is outside of this range, it's considered as fail \n",
    "            (KONSISTENSI_TC8a = 0). Otherwise, it's considered as pass (KONSISTENSI_TC8a = 1).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the new column with 1 (indicating pass) for all rows.\n",
    "    data['KONSISTENSI_TC8a'] = 1\n",
    "\n",
    "    # Identify the rows that meet the first condition: 'S20' should be equal to 2.\n",
    "    filtered_data_TC8a = data[(data['S20'] == 2)].copy()\n",
    "\n",
    "    # For the filtered rows, if 'S19' is outside the range '84111' to '84300', mark them as fail (KONSISTENSI_TC8a = 0).\n",
    "    data.loc[data.index.isin(filtered_data_TC8a.index) & (~data['S19'].astype(str).between('84111', '84300')), 'KONSISTENSI_TC8a'] = 0     \n",
    "    \n",
    "    return data\n",
    "\n",
    "result = validate_condition_TC8a(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397b4f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# Konsistensi T-C8b\n",
    "# -----------------------------------\n",
    "\n",
    "def validate_condition_TC8b(data):\n",
    "    \"\"\"\n",
    "    Validates the condition T-C8b.\n",
    "\n",
    "    Parameters:\n",
    "        data (pd.DataFrame): The data on which to perform the validation.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The data with an additional column indicating the \n",
    "                      results of the validation.\n",
    "                      \n",
    "    Explanation:\n",
    "        - First Condition: \n",
    "            The data is filtered based on the condition (data['S20'] == 4). \n",
    "            Rows that do not satisfy this condition are considered as pass \n",
    "            (KONSISTENSI_TC8b = 1).\n",
    "            \n",
    "        - Second Condition: \n",
    "            From the filtered data, if 'S19' is NaN (i.e., does not have a value), \n",
    "            it's considered as fail (KONSISTENSI_TC8b = 0). Otherwise, it's considered \n",
    "            as pass (KONSISTENSI_TC8b = 1).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the new column with 1 (indicating pass) for all rows.\n",
    "    data['KONSISTENSI_TC8b'] = 1\n",
    "\n",
    "    # Identify the rows that meet the first condition: 'S20' should be equal to 4.\n",
    "    filtered_data_TC8b = data[(data['S20'] == 4)].copy()\n",
    "\n",
    "    # For the filtered rows, if 'S19' is NaN, mark them as fail (KONSISTENSI_TC8b = 0).\n",
    "    data.loc[data.index.isin(filtered_data_TC8b.index) & data['S19'].isna(), 'KONSISTENSI_TC8b'] = 0\n",
    "    \n",
    "    return data\n",
    "\n",
    "result = validate_condition_TC8b(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205b96d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# Konsistensi T-C9\n",
    "# -----------------------------------\n",
    "\n",
    "def validate_condition_TC9(data):\n",
    "    \"\"\"\n",
    "    This function performs the T-C9 validation on the dataframe `data`.\n",
    "    \n",
    "    Parameters:\n",
    "        - data (pd.DataFrame): the input data to validate.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: the input dataframe with an additional column `KONSISTENSI_TC9`\n",
    "                      which indicates pass (1) or fail (0) for the validation.\n",
    "    \n",
    "    The validation conditions are as follows:\n",
    "        1. Filter rows with PKIS == 11. The rest are considered pass.\n",
    "        2. For the filtered rows from condition 1, if MSIC_1D is not 'T' and not NaN, mark them as fail (0).\n",
    "           If MSIC_1D is 'T' or NaN, they pass this validation stage (1).\n",
    "    \"\"\"\n",
    "    # Create a new column 'KONSISTENSI_TC9' and initialize it with 1 (pass)\n",
    "    data['KONSISTENSI_TC9'] = 1\n",
    "    \n",
    "    # First condition: Filter rows where 'PKIS' is 11\n",
    "    filtered_data_TC9 = data[data['PKIS'].astype(str) == '11'].copy()\n",
    "    \n",
    "    # Second condition:\n",
    "    # For the filtered rows, if 'MSIC_1D' is not 'T' and not NaN, mark them as fail (0).\n",
    "    data.loc[data.index.isin(filtered_data_TC9.index) & (data['MSIC_1D'] != 'T') & (data['MSIC_1D'].notna()), 'KONSISTENSI_TC9'] = 0          \n",
    "    \n",
    "    return data\n",
    "\n",
    "result = validate_condition_TC9(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80e3429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# Konsistensi T-C10\n",
    "# -----------------------------------\n",
    "\n",
    "def validate_condition_TC10(data):\n",
    "    \"\"\"\n",
    "    Validate the condition TC10 on the provided dataframe.\n",
    "\n",
    "    Condition:\n",
    "    1. First, filter the data where 'S19' is between '98100' and '98200'.\n",
    "    2. For the filtered data, if 'RIN' is NaN, it is considered pass (KONSISTENSI_TC10 = 1).\n",
    "       Else, it is considered fail (KONSISTENSI_TC10 = 0).\n",
    "\n",
    "    Parameters:\n",
    "        data (pd.DataFrame): The dataframe to validate.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The dataframe with the validation results.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure the 'S19' column is a string, so we can perform string comparisons\n",
    "    data['S19'] = data['S19'].astype(str)\n",
    "    \n",
    "    # First condition: Filter data where 'S19' is between '98100' and '98200'\n",
    "    filtered_data_TC10 = data[data['S19'].between('98100', '98200')].copy()\n",
    "    \n",
    "    # Initialize a new column for the validation results and set to 1 as default (pass)\n",
    "    data['KONSISTENSI_TC10'] = 1\n",
    "    \n",
    "    # Second condition: For the filtered rows, if 'RIN' is not NaN, mark them as fail (0)\n",
    "    data.loc[data.index.isin(filtered_data_TC10.index) & data['RIN'].notna(), 'KONSISTENSI_TC10'] = 0\n",
    "    \n",
    "    return data\n",
    "\n",
    "result = validate_condition_TC10(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe61ff5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# Konsistensi T-C11\n",
    "# -----------------------------------\n",
    "\n",
    "def validate_condition_TC11(data):\n",
    "    \"\"\"\n",
    "    Validate conditions for T-C11.\n",
    "    \n",
    "    The data is first filtered based on 'RIN' being between 1 and 5. Rows that do not\n",
    "    meet this condition are considered as 'pass' (KONSISTENSI_TC11 = 1).\n",
    "    \n",
    "    From the remaining filtered data, the function checks whether 'S19' equals '99000'.\n",
    "    If 'S19' is not '99000' or is NaN, it marks them as 'fail' (KONSISTENSI_TC11 = 0).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure the 'RIN' and 'S19' columns are treated as strings for consistency in comparison\n",
    "    data['RIN'] = data['RIN'].astype(str)\n",
    "    data['S19'] = data['S19'].astype(str)\n",
    "    \n",
    "    # First condition: filter rows where 'RIN' is between '1' and '5'.\n",
    "    filtered_data_TC11 = data[data['RIN'].isin(['1', '2', '3', '4', '5'])].copy()\n",
    "    \n",
    "    # Initialize the validation column with 'pass' (1).\n",
    "    data['KONSISTENSI_TC11'] = 1\n",
    "    \n",
    "    # Second condition:\n",
    "    # For the filtered rows, if 'S19' is not '99000' and not NaN, mark them as 'fail' (0).\n",
    "    data.loc[data.index.isin(filtered_data_TC11.index) & (~data['S19'].isin(['99000'])) & (data['S19'].notna()), 'KONSISTENSI_TC11'] = 0      \n",
    "    \n",
    "    return data\n",
    "\n",
    "result = validate_condition_TC11(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76aff0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# Konsistensi T-C12\n",
    "# -----------------------------------\n",
    "\n",
    "def validate_condition_TC12(data):\n",
    "    \"\"\"\n",
    "    Validate the consistency of the data against conditions (T-C12).\n",
    "    \n",
    "    This function checks the data for the following consistency conditions:\n",
    "    - First, it filters the data for rows where 'MASCO_1D' and 'MSIC_1D' have non-NaN values.\n",
    "    - Then, it checks whether 'S19' is between '98100' and '98200' (inclusive) for the filtered data.\n",
    "      If 'S19' is within this range, the data does not meet the consistency requirement.\n",
    "    \n",
    "    Parameters:\n",
    "    - data (pd.DataFrame): The data on which to perform the consistency check.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: The original data with an additional column 'KONSISTENSI_TC12',\n",
    "                  indicating pass (1) or fail (0) for each row based on the consistency check.\n",
    "    \"\"\"\n",
    "    \n",
    "    # First condition: \n",
    "    # Filter rows where both 'MASCO_1D' and 'MSIC_1D' have valid (non-NaN) values.\n",
    "    filtered_data_TC12 = data[data['MASCO_1D'].notna() & data['MSIC_1D'].notna()].copy()\n",
    "    \n",
    "    # Initialize a new column 'KONSISTENSI_TC12' with 1 (indicating pass) for all rows.\n",
    "    data['KONSISTENSI_TC12'] = 1\n",
    "\n",
    "    # Second condition: \n",
    "    # For the filtered rows, if 'S19' is within the range ['98100', '98200'], mark them as fail (0).\n",
    "    data.loc[data.index.isin(filtered_data_TC12.index) & data['S19'].between('98100', '98200'), 'KONSISTENSI_TC12'] = 0        \n",
    "\n",
    "    return data\n",
    "\n",
    "result = validate_condition_TC12(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0979ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# Konsistensi T-C13\n",
    "# -----------------------------------\n",
    "\n",
    "def validate_condition_TC13(data):\n",
    "    \"\"\"\n",
    "    Validate the data according to the T-C13 condition.\n",
    "    \n",
    "    Conditions:\n",
    "    1. First, filter the data where 'KW' is equal to 458. \n",
    "       If 'KW' is not equal to 458, the entry is considered as pass (KONSISTENSI_TC13 = 1).\n",
    "    2. From the filtered data, check if the 'KET' value is in the provided `ket_list`.\n",
    "       If 'KET' is NOT in `ket_list` or is NaN, it's considered a fail (KONSISTENSI_TC13 = 0).\n",
    "       Otherwise, it's considered a pass (KONSISTENSI_TC13 = 1).\n",
    "       \n",
    "    Parameters:\n",
    "    - data (DataFrame): The data on which to perform the validation.\n",
    "    - ket_list (list): A list of valid values for the 'KET' field.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: The data with an additional column 'KONSISTENSI_TC13' indicating pass/fail.\n",
    "    \"\"\"\n",
    "    # Ensure 'KW' and 'KET' are treated as strings\n",
    "    data['KW'] = data['KW'].astype(str)\n",
    "    data['KET'] = data['KET'].astype(str)\n",
    "    \n",
    "    # Initialize a new column for the validation results and set default as 1 (pass)\n",
    "    data['KONSISTENSI_TC13'] = 1\n",
    "    \n",
    "    # Identify rows that meet the first condition\n",
    "    condition_1 = (data['KW'] == '458')\n",
    "    \n",
    "    # For the rows meeting the first condition, if 'KET' is NOT in ket_list or is NaN, mark them as fail (0)\n",
    "    data.loc[condition_1 & (~data['KET'].isin(ket_list) | data['KET'].isna()), 'KONSISTENSI_TC13'] = 0\n",
    "    \n",
    "    return data\n",
    "\n",
    "result = validate_condition_TC13(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4652642c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# Konsistensi T-C14\n",
    "# -----------------------------------\n",
    "\n",
    "# TC-14: Jika Jantina (J) Lelaki (kod 1), \n",
    "#     check PKIS = 01 (Ketua Isi Rumah) dan S10 = 02 (kerja rumah/ tanggungjawab keluarga). \n",
    "#     Jika Umur (U) >= 50 boleh terima, tapi jika U < 50, semak semula\n",
    "\n",
    "def validate_condition_TC14(data):\n",
    "    \"\"\"\n",
    "    Validate the condition TC-14:\n",
    "    - First, filter rows where 'J' == '1', 'PKIS' == '01', and 'S10' == '2' (first condition).\n",
    "    - For the filtered rows, if 'U' < 50, mark them as fail (0).\n",
    "    \"\"\"\n",
    "    # Ensure the columns are in the correct data type\n",
    "    data['J'] = data['J'].astype(str)\n",
    "    data['PKIS'] = data['PKIS'].astype(str)\n",
    "    data['S10'] = data['S10'].astype(str)\n",
    "    data['U'] = pd.to_numeric(data['U'], errors='coerce')\n",
    "    \n",
    "    # Initialize a new column for the validation result and set default as 1 (pass)\n",
    "    data['KONSISTENSI_TC14'] = 1\n",
    "\n",
    "    # Filter the data to meet the first condition\n",
    "    filtered_data_TC14 = data[(data['J'] == '1') & (data['PKIS'] == '01') & (data['S10'] == '2')].copy()\n",
    "\n",
    "    # For the filtered rows, if 'U' < 50, mark them as fail (0)\n",
    "    data.loc[data.index.isin(filtered_data_TC14.index) & (data['U'] < 50), 'KONSISTENSI_TC14'] = 0\n",
    "\n",
    "    return data\n",
    "\n",
    "result = validate_condition_TC14(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccecea3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# Konsistensi T-C15a & T-C15b\n",
    "# -----------------------------------\n",
    "\n",
    "def validate_condition_TC15(data):\n",
    "    # Ensure the U column is treated as numeric\n",
    "    data['U'] = pd.to_numeric(data['U'], errors='coerce')\n",
    "    \n",
    "    # Initializing columns with 1 (pass)\n",
    "    data['KONSISTENSI_TC15a'] = 1\n",
    "    data['KONSISTENSI_TC15b'] = 1\n",
    "    \n",
    "    # First condition: T-C15a\n",
    "    # Filter where TP == 1 and PKIS is in ['02', '04', '05', '07', '08']\n",
    "    condition_TC15a = (data['TP'] == 1) & data['PKIS'].isin(['02', '04', '05', '07', '08'])\n",
    "    # Mark as fail (0) if the condition is met\n",
    "    data.loc[condition_TC15a, 'KONSISTENSI_TC15a'] = 0\n",
    "    \n",
    "    # Second condition: T-C15b\n",
    "    # Filter where TP in [2, 3, 4, 5] and U <= 17\n",
    "    condition_TC15b = data['TP'].isin([2, 3, 4, 5]) & (data['U'] <= 17)\n",
    "    # Mark as fail (0) if the condition is met\n",
    "    data.loc[condition_TC15b, 'KONSISTENSI_TC15b'] = 0\n",
    "    \n",
    "    # Combined condition: T-C15\n",
    "    # Initialize with 1 (pass)\n",
    "    data['KONSISTENSI_TC15'] = 1\n",
    "    # If either of the sub-conditions (T-C15a or T-C15b) is 0 (fail), mark T-C15 as 0 (fail)\n",
    "    data.loc[(data['KONSISTENSI_TC15a'] == 0) | (data['KONSISTENSI_TC15b'] == 0), 'KONSISTENSI_TC15'] = 0\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "result = validate_condition_TC15(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e88a574",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_condition_TC16a(data):\n",
    "    \"\"\"\n",
    "    Validate the consistency condition TC16a.\n",
    "    \n",
    "    Conditions:\n",
    "    - P = 1 (Not schooling)\n",
    "    - S10 = 01 and S15 = 1 OR S10 = 07 and S15 in [2, 3]\n",
    "    \n",
    "    If a row satisfies both conditions, it will be marked as fail (0).\n",
    "    Otherwise, it will be marked as pass (1).\n",
    "    \n",
    "    Parameters:\n",
    "        data (DataFrame): The data to validate.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: The data with a new column 'KONSISTENSI_TC16a' indicating pass/fail.\n",
    "    \"\"\"\n",
    "    # Filtering the data according to the specified conditions\n",
    "    filtered_data_TC16a = data[\n",
    "        (data['P'] == 1) &\n",
    "        (\n",
    "            ((data['S10'] == '01') & (data['S15'] == '1')) |\n",
    "            ((data['S10'] == '07') & data['S15'].isin(['2', '3']))\n",
    "        )\n",
    "    ].copy()\n",
    "    \n",
    "    # Initializing a new column with 1 (pass)\n",
    "    data['KONSISTENSI_TC16a'] = 1\n",
    "    \n",
    "    # Marking the rows that fail the conditions as 0 (fail)\n",
    "    data.loc[filtered_data_TC16a.index, 'KONSISTENSI_TC16a'] = 0\n",
    "\n",
    "    return data\n",
    "\n",
    "result = validate_condition_TC16a(df)\n",
    "\n",
    "################################################################################\n",
    "\n",
    "def validate_condition_TC16b(data):\n",
    "    \"\"\"\n",
    "    Validate the consistency condition TC16b.\n",
    "    \n",
    "    Conditions:\n",
    "    - P = 2 (Schooling)\n",
    "    - S10 in [03, 08, 09, 12, 13]\n",
    "    \n",
    "    If a row satisfies both conditions, it will be marked as fail (0).\n",
    "    Otherwise, it will be marked as pass (1).\n",
    "    \n",
    "    Parameters:\n",
    "        data (DataFrame): The data to validate.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: The data with a new column 'KONSISTENSI_TC16b' indicating pass/fail.\n",
    "    \"\"\"\n",
    "    # Filtering the data according to the specified conditions\n",
    "    filtered_data_TC16b = data[\n",
    "        (data['P'] == 2) & \n",
    "        data['S10'].isin(['03', '08', '09', '12', '13'])\n",
    "    ].copy()\n",
    "    \n",
    "    # Initializing a new column with 1 (pass)\n",
    "    data['KONSISTENSI_TC16b'] = 1\n",
    "    \n",
    "    # Marking the rows that fail the conditions as 0 (fail)\n",
    "    data.loc[filtered_data_TC16b.index, 'KONSISTENSI_TC16b'] = 0\n",
    "\n",
    "    return data\n",
    "\n",
    "result = validate_condition_TC16b(df)\n",
    "                               \n",
    "################################################################################\n",
    "\n",
    "def validate_condition_TC16c(data):\n",
    "    \"\"\"\n",
    "    Validate the consistency condition TC16c.\n",
    "    \n",
    "    Conditions:\n",
    "    - P = 4 (Finished school)\n",
    "    - S10 in [01, 07]\n",
    "    \n",
    "    If a row satisfies both conditions, it will be marked as fail (0).\n",
    "    Otherwise, it will be marked as pass (1).\n",
    "    \n",
    "    Parameters:\n",
    "        data (DataFrame): The data to validate.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: The data with a new column 'KONSISTENSI_TC16c' indicating pass/fail.\n",
    "    \"\"\"\n",
    "    # Filtering the data according to the specified conditions\n",
    "    filtered_data_TC16c = data[\n",
    "        (data['P'] == 4) & \n",
    "        data['S10'].isin(['01', '07'])\n",
    "    ].copy()\n",
    "    \n",
    "    # Initializing a new column with 1 (pass)\n",
    "    data['KONSISTENSI_TC16c'] = 1\n",
    "    \n",
    "    # Marking the rows that fail the conditions as 0 (fail)\n",
    "    data.loc[filtered_data_TC16c.index, 'KONSISTENSI_TC16c'] = 0\n",
    "\n",
    "    return data\n",
    "\n",
    "result = validate_condition_TC16c(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c9d3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data(df):\n",
    "    # Filter the data to get only rows that contain 'RIN' == 1\n",
    "    df = df[df['RIN'] == '1'].copy()\n",
    "\n",
    "    # Iterate over every row and check the conditions\n",
    "    results = []\n",
    "    for _, row in df.iterrows():\n",
    "        s8_result = row['S8'] in [1000, 200, 30, 4, 1004, 204, 34] if not np.isnan(row['S8']) else False\n",
    "        s18_result = len(str(row['S18'])) == 6\n",
    "        s19_result = len(str(row['S19'])) == 5\n",
    "        s20_result = row['S20'] in range(1, 7) if not np.isnan(row['S20']) else False\n",
    "\n",
    "        result = (\n",
    "            (row['S1'] == 1) and\n",
    "            (len(str(int(row['S3']))) == 2 and row['S3'] >= 30) and\n",
    "            (row['S7'] in [1, 2]) and\n",
    "            s8_result and\n",
    "            s18_result and\n",
    "            s19_result and\n",
    "            s20_result and\n",
    "            (row['S21'] in range(1, 4)) and\n",
    "            (row['S22'] in range(1, 4)) and\n",
    "            (row['S23'] in range(1, 12)) and\n",
    "            (row['S24'] in [1, 2]) and\n",
    "            (row['S34'] in [1, 2])\n",
    "        )\n",
    "        results.append(int(result))\n",
    "\n",
    "    # Add the results as a new column\n",
    "    df['combined_result'] = results\n",
    "\n",
    "    return df\n",
    "\n",
    "df = check_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fccd5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae97ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the original file name\n",
    "original_file_name = os.path.basename(dataset_path)\n",
    "\n",
    "suffix = '_konsistensi'\n",
    "\n",
    "# Create the new file name by adding the suffix and changing the extension to .xlsx\n",
    "new_file_name = original_file_name.replace('.xlsx', '') + suffix + '.xlsx'\n",
    "\n",
    "# Save the DataFrame to Excel using the new file name\n",
    "df.to_excel(os.path.join(output_file_path, new_file_name), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f015cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the original file name\n",
    "original_file_name = os.path.basename(dataset_path)\n",
    "\n",
    "suffix = '_konsistensi'\n",
    "\n",
    "# Create the new file name by adding the suffix\n",
    "new_file_name = original_file_name.replace('.csv', '') + suffix + '.csv'\n",
    "\n",
    "# Save the DataFrame as CSV using the new file name\n",
    "df.to_csv(os.path.join(output_file_path, new_file_name), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce76f74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openpyxl import load_workbook\n",
    "import yaml\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eef8523c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = r'C:/Users/aiman/Desktop/gh_konsistensi/'\n",
    "yaml_path = 'ref/stb_config_all_2021.yml'\n",
    "\n",
    "#http://localhost:8888/notebooks/Desktop/gh_konsistensi/src/stb/konsistensi_stb_master.ipynb# File paths\n",
    "dataset_path = base_path + 'data/stb/dsB092021STB.xlsx'\n",
    "reference_files = ['ref/oku.csv',\n",
    "                   'ref/kewarganegaraan.csv',\n",
    "                   'ref/kumpulan_etnik.csv', \n",
    "                   'ref/persekolahan.csv',\n",
    "                   'ref/pendidikan_rasmi.csv',\n",
    "                   'ref/pendidikan_rasmi_tertinggi_2022.csv', \n",
    "                   'ref/sijil_tertinggi.csv',\n",
    "                   'ref/sijil_tertinggi_2022.csv',\n",
    "                   'ref/status_code.csv', \n",
    "                   'ref/msic_code_detail_01.csv',\n",
    "                   'ref/masco_code.csv',\n",
    "                   'ref/negara_code.csv', \n",
    "                   'ref/institusi_pengajian.csv',\n",
    "                   'ref/bidang_pengajian.csv']\n",
    "\n",
    "file_paths = [dataset_path] + [base_path + file for file in reference_files]\n",
    "yaml_file = base_path + yaml_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7658405c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(*file_paths):\n",
    "    def read_csv(file_path):\n",
    "        return pd.read_csv(file_path, encoding='unicode_escape', low_memory=False)\n",
    "\n",
    "    def read_excel(file_path):\n",
    "        workbook = load_workbook(filename=file_path)\n",
    "        sheet_name = workbook.sheetnames[0]  # Get the name of the first sheet\n",
    "        worksheet = workbook[sheet_name]\n",
    "        data = list(worksheet.values)\n",
    "        return pd.DataFrame(data[1:], columns=data[0])\n",
    "\n",
    "    file_readers = {\n",
    "        'csv': read_csv,\n",
    "        'xlsx': read_excel,\n",
    "        'xls': read_excel\n",
    "    }\n",
    "    \n",
    "    predefined_names = {\n",
    "        'dsB092021STB.xlsx': 'df',\n",
    "        'oku.csv': 'df_oku',\n",
    "        'kewarganegaraan.csv': 'df_kw',\n",
    "        'kumpulan_etnik.csv': 'df_ket',\n",
    "        'persekolahan.csv': 'df_persk',\n",
    "        'pendidikan_rasmi.csv': 'df_pend',\n",
    "        'pendidikan_rasmi_tertinggi_2022.csv': 'df_pend_22',\n",
    "        'sijil_tertinggi.csv': 'df_sijil',\n",
    "        'sijil_tertinggi_2022.csv': 'df_sijil_22',\n",
    "        'status_code.csv': 'df_status',\n",
    "        'msic_code_detail_01.csv': 'df_msic',\n",
    "        'masco_code.csv': 'df_masco',\n",
    "        'negara_code.csv': 'df_ngra',\n",
    "        'institusi_pengajian.csv': 'df_ip',\n",
    "        'bidang_pengajian.csv': 'df_fs'\n",
    "    }\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        filename = file_path.split('/')[-1]\n",
    "        df_name = predefined_names[filename]\n",
    "        file_format = filename.split('.')[-1]\n",
    "        \n",
    "        # Declare the dataframe name as global\n",
    "        globals()[df_name] = file_readers[file_format](file_path)\n",
    "\n",
    "# Call the function\n",
    "read_files(*file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac3ec127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the YAML file\n",
    "with open(yaml_file, 'r') as file:\n",
    "    config = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "# Extract conditions from the YAML data\n",
    "persekolahan = config['persekolahan']\n",
    "\n",
    "# Create an empty dictionary to store the merged values\n",
    "merged_conditions = {}\n",
    "\n",
    "# Loop over the persekolahan dictionaries and merge the values\n",
    "for key, conditions in persekolahan.items():\n",
    "    merged_conditions[key] = {}\n",
    "    for condition_key, condition_value in conditions.items():\n",
    "        if condition_key == 'U':\n",
    "            if isinstance(condition_value, str) and condition_value.isdigit():\n",
    "                condition_value = int(condition_value)\n",
    "        merged_conditions[key][condition_key] = condition_value\n",
    "        \n",
    "for key in persekolahan:\n",
    "    persekolahan[key]['U'] = list(eval(persekolahan[key]['U']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "549c0095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the dataset name from the file path\n",
    "dataset_name = os.path.basename(dataset_path)\n",
    "\n",
    "# Extract the first three characters\n",
    "doc_type = dataset_name[:3]\n",
    "\n",
    "# Extract the characters at index 3 and 4\n",
    "quarter_ref = dataset_name[3:5]\n",
    "\n",
    "# Extract the characters from index 6 to 9\n",
    "year_ref = int(dataset_name[5:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cd0748f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the string \"None\" (and its variants with potential spaces) with NaN for the entire DataFrame\n",
    "for col in df.columns:\n",
    "    df[col] = df[col].apply(lambda x: np.nan if str(x).strip() == 'None' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e56d22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def format_columns(data, columns_to_format):\n",
    "#     # Standardize columns with 6 digits and leading zeros\n",
    "#     for column in columns_to_format['leading_zeros_6_digits']:\n",
    "#         if column in data.columns:\n",
    "#             data[column] = data[column].apply(lambda x: str(int(x)).zfill(6) if not pd.isnull(x) else x)\n",
    "#             print(f\"Formatted {column} to have 6 digits with leading zeros.\")\n",
    "#         else:\n",
    "#             print(f\"Column '{column}' not found in the DataFrame.\")\n",
    "\n",
    "#     # Standardize columns with 5 digits and leading zeros\n",
    "#     for column in columns_to_format['leading_zeros_5_digits']:\n",
    "#         if column in data.columns:\n",
    "#             data[column] = data[column].apply(lambda x: str(int(x)).zfill(5) if not pd.isnull(x) else x)\n",
    "#             print(f\"Formatted {column} to have 5 digits with leading zeros.\")\n",
    "#         else:\n",
    "#             print(f\"Column '{column}' not found in the DataFrame.\")\n",
    "            \n",
    "#     # Standardize columns with 5 digits and leading zeros\n",
    "#     for column in columns_to_format['leading_zeros_4_digits']:\n",
    "#         if column in data.columns:\n",
    "#             data[column] = data[column].apply(lambda x: str(int(x)).zfill(4) if not pd.isnull(x) else x)\n",
    "#             print(f\"Formatted {column} to have 4 digits with leading zeros.\")\n",
    "#         else:\n",
    "#             print(f\"Column '{column}' not found in the DataFrame.\")\n",
    "            \n",
    "#     # Standardize columns with 3 digits and leading zeros\n",
    "#     for column in columns_to_format['leading_zeros_3_digits']:\n",
    "#         if column in data.columns:\n",
    "#             data[column] = data[column].apply(lambda x: str(int(x)).zfill(3) if not pd.isnull(x) else x)\n",
    "#             print(f\"Formatted {column} to have 3 digits with leading zeros.\")\n",
    "#         else:\n",
    "#             print(f\"Column '{column}' not found in the DataFrame.\")\n",
    "            \n",
    "#     # Standardize columns with 2 digits and leading zeros\n",
    "#     for column in columns_to_format['leading_zeros_2_digits']:\n",
    "#         if column in data.columns:\n",
    "#             data[column] = data[column].apply(lambda x: str(int(x)).zfill(2) if not pd.isnull(x) else x)\n",
    "#             print(f\"Formatted {column} to have 2 digits with leading zeros.\")\n",
    "#         else:\n",
    "#             print(f\"Column '{column}' not found in the DataFrame.\")\n",
    "\n",
    "#     # Convert columns to specific formats\n",
    "#     for column, data_type in columns_to_format['data_types'].items():\n",
    "#         if column in data.columns:\n",
    "#             if data_type == int:\n",
    "#                 data[column] = data[column].apply(lambda x: int(x) if pd.notnull(x) and str(x).isdigit() else x)\n",
    "#                 print(f\"Converted {column} to {data_type}.\")\n",
    "#             else:\n",
    "#                 data[column] = data[column].astype(data_type)\n",
    "#                 print(f\"Converted {column} to {data_type}.\")\n",
    "#         else:\n",
    "#             print(f\"Column '{column}' not found in the DataFrame.\")\n",
    "\n",
    "#     # Standardize columns with 7 digits and two decimal places (string)\n",
    "#     for column in columns_to_format['standardize_7_digits']:\n",
    "#         if column in data.columns:\n",
    "#             data[column] = data[column].apply(lambda x: f\"{float(x):08.2f}\" if pd.notnull(x) and (isinstance(x, float) or (isinstance(x, str) and x.replace('.', '', 1).isdigit())) else x)\n",
    "#             print(f\"Standardized {column} to have 7 digits and two decimal places.\")\n",
    "#         else:\n",
    "#             print(f\"Column '{column}' not found in the DataFrame.\")\n",
    "\n",
    "#     # Return the modified DataFrame\n",
    "#     return data\n",
    "\n",
    "\n",
    "\n",
    "# columns_to_format_01 = {\n",
    "#     'leading_zeros_6_digits': [],\n",
    "#     'leading_zeros_5_digits': ['S19', 'S18'],\n",
    "#     'leading_zeros_4_digits': ['KW', 'NGRA', 'FS', 'MASCO_4D'],\n",
    "#     'leading_zeros_3_digits': ['KW', 'NGRA', 'FS', 'MASCO_3D', 'NO KEL'],\n",
    "#     'leading_zeros_2_digits': ['PKIS', 'BK', 'NGRI', 'PT', 'SJ', 'Grp_SJ', 'MASCO_2D'],\n",
    "#     'standardize_7_digits': [],\n",
    "#     'data_types': {\n",
    "#         'P': int,\n",
    "#         'U': int,\n",
    "#         'PT': str,\n",
    "#         'SJ': str,\n",
    "#         'HMIS': int,\n",
    "#         'HMWA': int,\n",
    "#         'MSIC_1D': str,\n",
    "#         'RIN': int,\n",
    "#         'B': int,\n",
    "#         'NG': int,\n",
    "#         'DP': int,\n",
    "#         'ST': int,\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# columns_to_format_02 = {\n",
    "#     'leading_zeros_6_digits': ['KOD_MASCO'],\n",
    "#     'leading_zeros_5_digits': [],\n",
    "#     'leading_zeros_4_digits': [],\n",
    "#     'leading_zeros_3_digits': [],\n",
    "#     'leading_zeros_2_digits': [],\n",
    "#     'standardize_7_digits': [],\n",
    "#     'data_types': {\n",
    "        \n",
    "#     }\n",
    "# }\n",
    "\n",
    "# columns_to_format_03 = {\n",
    "#     'leading_zeros_6_digits': [],\n",
    "#     'leading_zeros_5_digits': ['KOD_MSIC'],\n",
    "#     'leading_zeros_4_digits': [],\n",
    "#     'leading_zeros_3_digits': [],\n",
    "#     'leading_zeros_2_digits': [],\n",
    "#     'standardize_7_digits': [],\n",
    "#     'data_types': {\n",
    "        \n",
    "#     }\n",
    "# }\n",
    "\n",
    "# columns_to_format_04 = {\n",
    "#     'leading_zeros_6_digits': ['Kod'],\n",
    "#     'leading_zeros_5_digits': [],\n",
    "#     'leading_zeros_4_digits': [],\n",
    "#     'leading_zeros_3_digits': [],\n",
    "#     'leading_zeros_2_digits': [],\n",
    "#     'standardize_7_digits': [],\n",
    "#     'data_types': {\n",
    "        \n",
    "#     }\n",
    "# }\n",
    "\n",
    "# formatted_data_01 = format_columns(df, columns_to_format_01)\n",
    "# formatted_data_02 = format_columns(df_masco, columns_to_format_02)\n",
    "# formatted_data_03 = format_columns(df_msic, columns_to_format_03)\n",
    "# formatted_data_04 = format_columns(df_status, columns_to_format_04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78976aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_list = df_kw[\"Kod\"].astype(str).apply(lambda x: x.zfill(3)).tolist()\n",
    "ket_list = list(map(int, df_ket.iloc[:97][\"Kod\"].values))\n",
    "status_list = df_status[\"Kod\"].values.tolist()\n",
    "masco_list = df_masco[\"KOD_MASCO\"].values.tolist()\n",
    "msic_list = df_msic[\"KOD_MSIC\"].values.tolist()\n",
    "pkis_list = [str(i).zfill(2) for i in range(1, 13)]\n",
    "no_kel_list = [str(i).zfill(3) for i in range(1, 1000)]\n",
    "ngri_list = df_ngra[\"KOD\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf4e7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Semakan Julat JR4 ###\n",
    "\n",
    "kel_list = list(range(1000))\n",
    "b_list = list(range(13))\n",
    "ng_list = list(range(17))\n",
    "dp_list = list(range(32))\n",
    "db_list = list(range(170))\n",
    "db2_list = set(str(i).zfill(5) for i in range(1, 100000))\n",
    "st_list = list(range(10))\n",
    "notk_list = list(range(1000))\n",
    "noir_list = list(range(100))\n",
    "t_list = list(range(100))\n",
    "pkis_list = [str(i).zfill(2) for i in range(1, 13)]\n",
    "hmis_list = [str(i).zfill(2) for i in range(1, 100)]\n",
    "bk_list = [str(i).zfill(2) for i in range(1, 13)]\n",
    "tk_list = [str(i).zfill(4) for i in range(1900, 3000)]\n",
    "u_list = list(range(201))\n",
    "tp_list = list(range(6))\n",
    "ngri_list = [str(i).zfill(2) for i in range(1, 17)] + ['98']\n",
    "ngra_list = df_ngra[\"KOD\"].astype(str).apply(lambda x: x.zfill(3)).tolist()\n",
    "oku_list = df_oku[\"Kod\"].astype(str).apply(lambda x: x.zfill(2)).tolist()\n",
    "pt_22_list = df_pend_22[\"KOD\"].astype(str).apply(lambda x: x.zfill(3)).tolist()\n",
    "sj_22_list = df_sijil_22[\"KOD\"].astype(str).apply(lambda x: x.zfill(3)).tolist()\n",
    "ip_list = df_ip[\"Kod\"].tolist()\n",
    "fs_list = df_fs[\"Kod\"].astype(str).apply(lambda x: x.zfill(4)).tolist()\n",
    "hmwa_list = list(range(100))\n",
    "\n",
    "def validate_all_julats(row):\n",
    "    row['JULAT_001'] = 1 if row['NOKEL'] in no_kel_list else 0\n",
    "    row['JULAT_002'] = 1 if row['B'] in b_list else 0\n",
    "    row['JULAT_003'] = 1 if row['NG'] in ng_list else 0\n",
    "    row['JULAT_004'] = 1 if row['DP'] in dp_list else 0\n",
    "    row['JULAT_005'] = 1 if row['DB'] in db_list else 0\n",
    "    row['JULAT_006'] = 1 if row['BP'] in db_list else 0\n",
    "    row['JULAT_007'] = 1 if row['BP2'] in db2_list else 0\n",
    "    row['JULAT_008'] = 1 if row['ST'] in st_list else 0\n",
    "    row['JULAT_009'] = 1 if row['NOTK'] in notk_list else 0\n",
    "    row['JULAT_010'] = 1 if row['NOIR'] in noir_list else 0\n",
    "    row['JULAT_011'] = 1 if row['JR'] == 4 else 0\n",
    "    row['JULAT_012'] = 1 if isinstance(row['NAMA'], str) and len(row['NAMA']) <= 50 else 0\n",
    "    row['JULAT_013'] = 1 if isinstance(row['NOIC'], str) and len(row['NOIC']) <= 12 or pd.isna(row['NOIC']) else 0\n",
    "    row['JULAT_014'] = 1 if row['PKIS'] in pkis_list else 0\n",
    "    row['JULAT_015'] = 1 if row['HMIS'] in hmis_list else 0\n",
    "    row['JULAT_016'] = 1 if row['J'] in [1, 2] else 0\n",
    "    row['JULAT_017'] = 1 if row['BK'] in bk_list else 0\n",
    "    row['JULAT_018'] = 1 if row['TK'] in tk_list else 0\n",
    "    row['JULAT_019'] = 1 if row['U'] in u_list else 0\n",
    "    row['JULAT_020'] = 1 if row['KET'] in ket_list else 0\n",
    "    row['JULAT_021'] = 1 if row['KW'] in kw_list else 0\n",
    "    row['JULAT_022'] = 1 if row['TP'] in tp_list else 0\n",
    "    row['JULAT_023'] = 1 if row['NGRI'] in pkis_list else 0\n",
    "    row['JULAT_024'] = 1 if row['NGRA'] in ngra_list else 0\n",
    "    row['JULAT_025'] = 1 if row['OKU'] in oku_list else 0\n",
    "    row['JULAT_026'] = 1 if row['P'] in [1, 2, 3, 4] else 0\n",
    "    row['JULAT_027'] = 1 if row['PT'] in pt_22_list else 0\n",
    "    row['JULAT_028'] = 1 if row['SJ'] in sj_22_list else 0\n",
    "    row['JULAT_029'] = 1 if row['IP'] in ip_list or pd.isna(row['IP']) else 0\n",
    "    row['JULAT_030'] = 1 if row['FS'] in fs_list else 0\n",
    "    row['JULAT_031'] = 1 if row['HMWA'] in hmwa_list else 0\n",
    "    \n",
    "    return row\n",
    "\n",
    "df = df.apply(validate_all_julats, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d9e003f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aiman\\AppData\\Local\\Temp\\ipykernel_13440\\2916898231.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['KONSISTENSI_01a'] = df.apply(lambda row: validate_condition_01(row) if row['TK'].isdigit() and row['U'].isdigit() else 0, axis=1).astype(int)\n"
     ]
    }
   ],
   "source": [
    "### Konsistensi 1(a) ###\n",
    "\n",
    "# Create today date variable\n",
    "# today = datetime.date.today()\n",
    "# tahun_semasa = today.year\n",
    "\n",
    "def validate_condition_01(row):\n",
    "    \"\"\"\n",
    "    Validate the consistency between year of birth, age, and reference year.\n",
    "    \n",
    "    This function takes a row of a DataFrame as input, extracts the values of\n",
    "    'TK' and 'U', converts them to integers, and checks whether the \n",
    "    expression (year_ref - TK - U) is greater than 3. If the expression \n",
    "    is true, it returns 0, indicating that the data is inconsistent. If the\n",
    "    expression is false, it returns 1, indicating consistency.\n",
    "\n",
    "    Parameters:\n",
    "        row (pd.Series): A row of a DataFrame, expected to contain \n",
    "                         'TK' and 'U' keys with digit strings as values.\n",
    "        \n",
    "    Returns:\n",
    "        int: Returns 1 if the data is consistent (i.e., (year_ref - TK - U) <= 3),\n",
    "             and 0 otherwise.\n",
    "    \"\"\"\n",
    "    # Calculate and check the condition (year_ref - TK - U) > 3.\n",
    "    # If true, return 0 (inconsistent data). Otherwise, return 1 (consistent data).\n",
    "    if (year_ref - int(row['TK']) - int(row['U'])) > 3:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "\n",
    "df['KONSISTENSI_01a'] = df.apply(lambda row: validate_condition_01(row) if row['TK'].isdigit() and row['U'].isdigit() else 0, axis=1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b7124f7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aiman\\AppData\\Local\\Temp\\ipykernel_13440\\460141923.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['KONSISTENSI_02'] = 0\n"
     ]
    }
   ],
   "source": [
    "### Konsistensi 1(b) ###\n",
    "def validate_condition_02(df):\n",
    "    \"\"\"\n",
    "    Sequential in-place validation of the DataFrame based on multiple conditions without any return:\n",
    "    1. Check if 'U' <= 15. If true, set 'KONSISTENSI_03' to 1.\n",
    "    2. If 'U' > 15, check if 'HMWA' is not null.\n",
    "    \n",
    "    Adds a 'KONSISTENSI_03' column with 0 (False) or 1 (True) based on the conditions.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: Input DataFrame\n",
    "    \"\"\"\n",
    "    # Initialize KONSISTENSI_03 column with 0\n",
    "    df['KONSISTENSI_02'] = 0\n",
    "    \n",
    "    # Directly set rows where 'U' <= 15 to 1\n",
    "    df.loc[df['U'].astype(int) <= 15, 'KONSISTENSI_02'] = 1\n",
    "    \n",
    "    # For rows where 'U' > 15 and 'HMWA' is not null, set 'KONSISTENSI_03' to 1\n",
    "    combined_condition = (df['U'].astype(int) > 15) & ~df['HMWA'].isnull()\n",
    "    df.loc[combined_condition, 'KONSISTENSI_02'] = 1\n",
    "\n",
    "validate_condition_02(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "88212541",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aiman\\AppData\\Local\\Temp\\ipykernel_13440\\266756301.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['KONSISTENSI_03'] = 1\n"
     ]
    }
   ],
   "source": [
    "### Konsistensi 1(c) ###\n",
    "\n",
    "# Define the list of allowed values for the 'NGRI' column\n",
    "ngri_lst = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14', '15', '16']\n",
    "\n",
    "# Main function\n",
    "def validate_condition_03(df, values):\n",
    "    # Create a new column 'KONSISTENSI_03' with initial value as 1 (Pass)\n",
    "    df['KONSISTENSI_03'] = 1\n",
    "\n",
    "    # Filter DataFrame for rows where 'NGRA' equals 458\n",
    "    df_ngra = df[df['NGRA'] == '458']\n",
    "\n",
    "    # Within this filtered DataFrame, find rows where 'NGRI' is not in the allowed values\n",
    "    mask = ~df_ngra['NGRI'].isin(values)\n",
    "\n",
    "    # Where the condition is met, update 'KONSISTENSI_03' to 0 (Fail) in the original DataFrame\n",
    "    df.loc[df_ngra[mask].index, 'KONSISTENSI_03'] = 0\n",
    "\n",
    "    return df\n",
    "\n",
    "df = validate_condition_03(df, ngri_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "14fe9d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aiman\\AppData\\Local\\Temp\\ipykernel_13440\\3730047015.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['KONSISTENSI_04'] = df['OKU'].astype(int).isin(oku_list).astype(int)\n"
     ]
    }
   ],
   "source": [
    "### Konsistensi 1(d) ###\n",
    "def validate_condition_04(df, oku_list):\n",
    "    \"\"\"\n",
    "    Validate the DataFrame based on the provided list of OKU values:\n",
    "    1. Check if the 'OKU' column contains a value present in oku_list.\n",
    "    \n",
    "    Adds a 'KONSISTENSI_04' column with 0 (False) or 1 (True) based on the condition.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: Input DataFrame\n",
    "    - oku_list: List of valid OKU values\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with 'OKU_VALIDATION' column added\n",
    "    \"\"\"\n",
    "    # Check if values in 'OKU' are present in oku_list and assign the results to 'OKU_VALIDATION'\n",
    "    df['KONSISTENSI_04'] = df['OKU'].astype(int).isin(oku_list).astype(int)\n",
    "\n",
    "oku_list = list(map(int, df_oku[\"Kod\"].values))\n",
    "\n",
    "validate_condition_04(df, oku_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7e4860a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aiman\\AppData\\Local\\Temp\\ipykernel_13440\\2854564991.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['KONSISTENSI_05a'] = df['KW'].astype(str).isin(kw).astype(int)\n",
      "C:\\Users\\aiman\\AppData\\Local\\Temp\\ipykernel_13440\\2854564991.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['KONSISTENSI_05b'] = df['KET'].astype(int).isin(ket).astype(int)\n"
     ]
    }
   ],
   "source": [
    "### Konsistensi 1(e) ###\n",
    "\n",
    "# Main function\n",
    "def validate_condition_05a(df, kw):\n",
    "    df['KONSISTENSI_05a'] = df['KW'].astype(str).isin(kw).astype(int)\n",
    "\n",
    "validate_condition_05a(df, kw_list)\n",
    "\n",
    "# Main function\n",
    "def validate_condition_05b(df, ket):\n",
    "    df['KONSISTENSI_05b'] = df['KET'].astype(int).isin(ket).astype(int)\n",
    "\n",
    "validate_condition_05b(df, ket_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3a0ba792",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aiman\\AppData\\Local\\Temp\\ipykernel_13440\\2174362371.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['KONSISTENSI_06'] = 0\n"
     ]
    }
   ],
   "source": [
    "### Konsistensi 1(f) ###\n",
    "\n",
    "# Main function\n",
    "def validate_condition_06(df, merged_conditions):\n",
    "    # Create a new column to store the result\n",
    "    df['KONSISTENSI_06'] = 0\n",
    "\n",
    "    # Loop over the rows in the dataframe\n",
    "    for index, row in df.iterrows():\n",
    "        # Check if the row matches any of the conditions\n",
    "        for condition_key, condition_values in merged_conditions.items():\n",
    "            # Flag to track if all conditions are met\n",
    "            match = True\n",
    "\n",
    "            # Check if all column-value pairs in the condition are present in the row\n",
    "            for col, val in condition_values.items():\n",
    "                # Check if the value is a list\n",
    "                if isinstance(val, list):\n",
    "                    if row[col] not in val:\n",
    "                        match = False\n",
    "                        break\n",
    "                else:\n",
    "                    if row[col] != val:\n",
    "                        match = False\n",
    "                        break\n",
    "\n",
    "            # Set the result column to 1 if all conditions are met\n",
    "            if match:\n",
    "                df.loc[index, 'KONSISTENSI_06'] = 1\n",
    "                break\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df = validate_condition_06(df, persekolahan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69fd67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Konsistensi 1(g) ###\n",
    "\n",
    "###\n",
    "# Same as 1(f)\n",
    "###\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "181ca08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_list = list(range(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "adbff027",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aiman\\AppData\\Local\\Temp\\ipykernel_13440\\140578832.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['KONSISTENSI_08'] = 0\n"
     ]
    }
   ],
   "source": [
    "def validate_condition_08(df, tp_lst):\n",
    "    \"\"\"\n",
    "    Sequential in-place validation of the DataFrame based on multiple conditions without any return:\n",
    "    1. Check if 'U' >= 18. If true, set 'KONSISTENSI_08' to 1.\n",
    "    2. If 'U' < 18, check if 'TP' is in tp_lst.\n",
    "    \n",
    "    Adds a 'KONSISTENSI_08' column with 0 (False) or 1 (True) based on the conditions.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: Input DataFrame\n",
    "    - tp_lst: List of valid TP values\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize KONSISTENSI_08 column with 0\n",
    "    df['KONSISTENSI_08'] = 0\n",
    "    \n",
    "    # Directly set rows where 'U' >= 18 to 1\n",
    "    df.loc[df['U'].astype(int) >= 18, 'KONSISTENSI_08'] = 1\n",
    "    \n",
    "    # For rows where 'U' < 18 and 'TP' is in tp_lst, set 'KONSISTENSI_08' to 1\n",
    "    combined_condition = (df['U'].astype(int) < 18) & df['TP'].isin(tp_lst)\n",
    "    df.loc[combined_condition, 'KONSISTENSI_08'] = 1\n",
    "\n",
    "validate_condition_08(df, tp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7cde0810",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aiman\\AppData\\Local\\Temp\\ipykernel_13440\\3990932040.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['KONSISTENSI_09'] = 0\n"
     ]
    }
   ],
   "source": [
    "def validate_condition_09(df):\n",
    "    \"\"\"\n",
    "    Sequential in-place validation of the DataFrame based on multiple conditions without any return:\n",
    "    1. Check if 'U' > 15 and 'HMWA' is not null.\n",
    "    2. If the first condition is met, check if 'HMWA' and 'HMIS' values are equal.\n",
    "    \n",
    "    Adds a 'KONSISTENSI_09' column with 0 (False) or 1 (True) based on the conditions.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: Input DataFrame\n",
    "    \"\"\"\n",
    "    # Initialize VALIDATION_RESULT column with 0\n",
    "    df['KONSISTENSI_09'] = 0\n",
    "    \n",
    "    # Identify rows that meet the first condition\n",
    "    condition_1 = (df['U'].astype(int) > 15) & ~df['HMWA'].isnull()\n",
    "    \n",
    "    # For rows that meet the first condition, check the second condition\n",
    "    condition_2 = df['HMWA'] == df['HMIS']\n",
    "    \n",
    "    # Update VALIDATION_RESULT column for rows that meet both conditions\n",
    "    df.loc[condition_1 & condition_2, 'KONSISTENSI_09'] = 1\n",
    "\n",
    "validate_condition_09(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "75a8a694",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KONSISTENSI_09\n",
       "1    20343\n",
       "0     6822\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['KONSISTENSI_09'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d25bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### T-C1a ###\n",
    "\n",
    "# Main function\n",
    "def validate_condition_TC1a(data):\n",
    "    # Apply the condition filters\n",
    "    filtered_data_TC1a = data[(data['MSIC_1D'] == 'O') &\n",
    "                             ~((data['S19'] >= '84111') & (data['S19'] <= '84300'))].copy()\n",
    "\n",
    "    # Create a new column 'KONSISTENSI_TC1a' and initialize it with 1 (pass)\n",
    "    data['KONSISTENSI_TC1a'] = 1\n",
    "\n",
    "    # Mark the rows that fail the condition as 0 (fail)\n",
    "    data.loc[filtered_data_TC1a.index, 'KONSISTENSI_TC1a'] = 0\n",
    "\n",
    "    return data\n",
    "\n",
    "result = validate_condition_TC1a(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febd3393",
   "metadata": {},
   "outputs": [],
   "source": [
    "### T-C1b ###\n",
    "\n",
    "def validate_condition_TC1b(data):\n",
    "    # Apply the condition filters\n",
    "    filtered_data_TC1b = data[(data['MSIC_1D'] == 'O') &\n",
    "                            (data['S19'] >= '84111') & (data['S19'] <= '84300') & \n",
    "                            (data['KW'] != 458)].copy()\n",
    "\n",
    "    # Create a new column 'validation_03' and initialize it with 1 (pass)\n",
    "    data['KONSISTENSI_TC1b'] = 1\n",
    "\n",
    "    # Mark the rows that fail the condition as 0 (fail)\n",
    "    data.loc[filtered_data_TC1b.index, 'KONSISTENSI_TC1b'] = 0\n",
    "\n",
    "    return data\n",
    "\n",
    "result = validate_condition_TC1b(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123cafff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_msic = df[df['MSIC_1D'] == 'T']\n",
    "df_msic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1d0209",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_condition_TC2(df):\n",
    "    \"\"\"\n",
    "    Sequential in-place validation of the DataFrame based on multiple conditions without any return:\n",
    "    1. Check if 'MSIC_1D' == T.\n",
    "    2. If the first condition is met, check if 'S20' == 3.\n",
    "    \n",
    "    Adds a 'KONSISTENSI_TC2' column with 0 (False) or 1 (True) based on the conditions.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: Input DataFrame\n",
    "    \"\"\"\n",
    "    # Initialize VALIDATION_RESULT column with 0\n",
    "    df['KONSISTENSI_TC2'] = 1\n",
    "    \n",
    "    # Identify rows that meet the first condition\n",
    "    condition_1 = (df['MSIC_1D'] == 'T')\n",
    "    \n",
    "    # For rows that meet the first condition, check the second condition\n",
    "    condition_2 = df['S20'] != 3\n",
    "    \n",
    "    # Update VALIDATION_RESULT column for rows that meet both conditions\n",
    "    df.loc[condition_1 & condition_2, 'KONSISTENSI_TC2'] = 1\n",
    "    \n",
    "validate_condition_TC2(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed21ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### T-C3 ###\n",
    "\n",
    "def validate_condition_TC3(data):\n",
    "    # Apply the condition filters\n",
    "    filtered_data_04 = data[(data['MSIC_1D'] == 'P') & (data['S20'] != 2)].copy()\n",
    "\n",
    "    # Create a new column 'validation_03' and initialize it with 1 (pass)\n",
    "    data['KONSISTENSI_TC3'] = 1\n",
    "\n",
    "    # Mark the rows that fail the condition as 0 (fail)\n",
    "    data.loc[filtered_data_04.index, 'KONSISTENSI_TC3'] = 0\n",
    "\n",
    "    return data\n",
    "\n",
    "result = validate_condition_TC3(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae5ed93",
   "metadata": {},
   "outputs": [],
   "source": [
    "### T-C4 ###\n",
    "\n",
    "def validate_condition_TC4(data):\n",
    "    # Apply the condition filters\n",
    "    filtered_data_TC4 = data[(data['MSIC_1D'] == 'A') &\n",
    "                            (data['MASCO_1D'] == 9) &\n",
    "                            (~data['S20'].isin([3, 4, 5]))].copy()\n",
    "\n",
    "    # Create a new column 'validation_03' and initialize it with 1 (pass)\n",
    "    data['KONSISTENSI_TC4'] = 1\n",
    "\n",
    "    # Mark the rows that fail the condition as 0 (fail)\n",
    "    data.loc[filtered_data_TC4.index, 'KONSISTENSI_TC4'] = 0\n",
    "\n",
    "    return data\n",
    "\n",
    "result = validate_condition_TC4(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e4be85",
   "metadata": {},
   "outputs": [],
   "source": [
    "### T-C5a ###\n",
    "\n",
    "def validate_condition_TC5a(data):\n",
    "    data['SJ'] = pd.to_numeric(data['SJ'], errors='coerce')\n",
    "    \n",
    "    # Apply the condition filters\n",
    "    filtered_data_TC5a = data[((data['S18'] >= '111101') &\n",
    "                            (data['S18'] <= '291918')) &\n",
    "                            (~(data['SJ'] >= 20) &\n",
    "                            (data['SJ'] <= 242))].copy()\n",
    "\n",
    "    # Create a new column 'validation_03' and initialize it with 1 (pass)\n",
    "    data['KONSISTENSI_TC5a'] = 1\n",
    "\n",
    "    # Mark the rows that fail the condition as 0 (fail)\n",
    "    data.loc[filtered_data_TC5a.index, 'KONSISTENSI_TC5a'] = 0\n",
    "\n",
    "    return data\n",
    "\n",
    "result = validate_condition_TC5a(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1876c2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### T-C5b ###\n",
    "\n",
    "def validate_condition_TC5b(data):\n",
    "    # Apply the condition filters\n",
    "    filtered_data_TC5b = data[~(data['S20'] == 4) &\n",
    "                            ((data['S18'] >= '111101') & (data['S18'] <= '291918'))].copy()\n",
    "\n",
    "    # Create a new column 'validation_03' and initialize it with 1 (pass)\n",
    "    data['KONSISTENSI_TC5b'] = 1\n",
    "\n",
    "    # Mark the rows that fail the condition as 0 (fail)\n",
    "    data.loc[filtered_data_TC5b.index, 'KONSISTENSI_TC5b'] = 0\n",
    "\n",
    "    return data\n",
    "\n",
    "result = validate_condition_TC5b(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4091ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "### T-C6 ###\n",
    "\n",
    "def validate_condition_TC6(data):\n",
    "    # Apply the condition filters\n",
    "    filtered_data_TC7 = data[(data['STATUS'] == 'GOV') &\n",
    "                            ~(data['S20'] == 2)].copy()\n",
    "\n",
    "    # Create a new column 'validation_03' and initialize it with 1 (pass)\n",
    "    data['KONSISTENSI_TC6'] = 1\n",
    "\n",
    "    # Mark the rows that fail the condition as 0 (fail)\n",
    "    data.loc[filtered_data_TC7.index, 'KONSISTENSI_TC6'] = 0\n",
    "\n",
    "    return data\n",
    "\n",
    "result = validate_condition_TC6(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c543591",
   "metadata": {},
   "outputs": [],
   "source": [
    "### T-C7 ###\n",
    "\n",
    "def validate_condition_TC7(data, masco_list):\n",
    "    # Apply the condition filters\n",
    "    filtered_data_TC7 = data[(data['S18'].isin(masco_list)) & \n",
    "                              (data['S20'] != 2) &\n",
    "                              (data['STATUS'] != 'GOV')].copy()\n",
    "\n",
    "    # Create a new column 'validation_03' and initialize it with 1 (pass)\n",
    "    data['KONSISTENSI_TC7'] = 1\n",
    "\n",
    "    # Mark the rows that meet the condition as 0 (fail)\n",
    "    data.loc[filtered_data_TC7.index, 'KONSISTENSI_TC7'] = 0\n",
    "\n",
    "    return data  # return the dataframe\n",
    "\n",
    "result = validate_condition_TC7(df, masco_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872f25c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### T-C8a ###\n",
    "\n",
    "def validate_condition_TC8a(data):\n",
    "    # Apply the condition filters\n",
    "    filtered_data_TC8a = data[(data['S20'] == 2) &\n",
    "                            ~((data['S19'] >= '84111') & (data['S19'] <= '84300'))].copy()\n",
    "\n",
    "    # Create a new column 'validation_03' and initialize it with 1 (pass)\n",
    "    data['KONSISTENSI_TC8a'] = 1\n",
    "\n",
    "    # Mark the rows that fail the condition as 0 (fail)\n",
    "    data.loc[filtered_data_TC8a.index, 'KONSISTENSI_TC8a'] = 0\n",
    "\n",
    "    return data\n",
    "\n",
    "result = validate_condition_TC8a(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397b4f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "### T-C8b ###\n",
    "\n",
    "def validate_condition_TC8b(data):\n",
    "    # Apply the condition filters\n",
    "    filtered_data_TC8b = data[(data['S20'] == 4) &\n",
    "                              data['S19'].notna()].copy()\n",
    "\n",
    "    # Create a new column 'validation_03' and initialize it with 1 (pass)\n",
    "    data['KONSISTENSI_TC8b'] = 1\n",
    "\n",
    "    # Mark the rows that fail the condition as 0 (fail)\n",
    "    data.loc[filtered_data_TC8b.index, 'KONSISTENSI_TC8b'] = 0\n",
    "\n",
    "    return data\n",
    "\n",
    "result = validate_condition_TC8b(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205b96d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### T-C9 ###\n",
    "\n",
    "def validate_condition_TC9(data):\n",
    "    # Apply the condition filters\n",
    "    filtered_data_TC9 = data[(data['MSIC_1D'] == 'T') &\n",
    "                            (data['PKIS'] != 11)].copy()\n",
    "\n",
    "    # Create a new column 'validation_03' and initialize it with 1 (pass)\n",
    "    data['KONSISTENSI_TC9'] = 1\n",
    "\n",
    "    # Mark the rows that fail the condition as 0 (fail)\n",
    "    data.loc[filtered_data_TC9.index, 'KONSISTENSI_TC9'] = 0\n",
    "\n",
    "    return data\n",
    "\n",
    "result = validate_condition_TC9(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80e3429",
   "metadata": {},
   "outputs": [],
   "source": [
    "### T-C10 ###\n",
    "\n",
    "def validate_condition_TC10(data):\n",
    "    # Apply the condition filters\n",
    "    filtered_data_TC10 = data[((data['S19'] >= '98100') & (data['S19'] <= '98200')) &\n",
    "                            ~(data['RIN'] != np.NaN)].copy()\n",
    "\n",
    "    # Create a new column 'validation_03' and initialize it with 1 (pass)\n",
    "    data['KONSISTENSI_TC10'] = 1\n",
    "\n",
    "    # Mark the rows that fail the condition as 0 (fail)\n",
    "    data.loc[filtered_data_TC10.index, 'KONSISTENSI_TC10'] = 0\n",
    "\n",
    "    return data\n",
    "\n",
    "result = validate_condition_TC10(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe61ff5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### T-C11 ###\n",
    "\n",
    "def validate_condition_TC11(data):\n",
    "    # Apply the condition filters\n",
    "    filtered_data_TC11 = data[((data['RIN'] >= '1') & (data['RIN'] <= '5')) &\n",
    "                            ~(data['S19'] != '99000')].copy()\n",
    "\n",
    "    # Create a new column 'validation_03' and initialize it with 1 (pass)\n",
    "    data['KONSISTENSI_TC11'] = 1\n",
    "\n",
    "    # Mark the rows that fail the condition as 0 (fail)\n",
    "    data.loc[filtered_data_TC11.index, 'KONSISTENSI_TC11'] = 0\n",
    "\n",
    "    return data\n",
    "\n",
    "result = validate_condition_TC11(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76aff0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### T-C12 ###\n",
    "\n",
    "def validate_condition_TC12(data):\n",
    "    # Apply the condition filters\n",
    "    filtered_data_TC12 = data[(data['MASCO_1D'] != np.NaN) &\n",
    "                            (data['MSIC_1D'] != np.NaN) &\n",
    "                            (~(data['S19'] >= '98100') & (data['S19'] <= '98200'))].copy()\n",
    "    \n",
    "    # Create a new column 'validation_03' and initialize it with 1 (pass)\n",
    "    data['KONSISTENSI_TC12'] = 1\n",
    "\n",
    "    # Mark the rows that fail the condition as 0 (fail)\n",
    "    data.loc[filtered_data_TC12.index, 'KONSISTENSI_TC12'] = 0\n",
    "\n",
    "    return data\n",
    "\n",
    "result = validate_condition_TC12(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0979ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### T-C13 ###\n",
    "\n",
    "def validate_condition_TC13(data):\n",
    "    # Apply the condition filters\n",
    "    filtered_data_TC13 = data[(data['KW'] == '458') &\n",
    "                            ~(data['KET'].isin(ket_list))].copy()\n",
    "\n",
    "    # Create a new column 'validation_03' and initialize it with 1 (pass)\n",
    "    data['KONSISTENSI_TC13'] = 1\n",
    "\n",
    "    # Mark the rows that fail the condition as 0 (fail)\n",
    "    data.loc[filtered_data_TC13.index, 'KONSISTENSI_TC13'] = 0\n",
    "\n",
    "    return data\n",
    "\n",
    "result = validate_condition_TC13(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4652642c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### T-C14 ###\n",
    "\n",
    "# TC-14: Jika Jantina (J) Lelaki (kod 1), \n",
    "#     check PKIS = 01 (Ketua Isi Rumah) dan S10 = 02 (kerja rumah/ tanggungjawab keluarga). \n",
    "#     Jika Umur (U) >= 50 boleh terima, tapi jika U < 50, semak semula\n",
    "\n",
    "\n",
    "def validate_condition_TC14(data):\n",
    "    # Apply the condition filters\n",
    "    filtered_data_TC14 = data[(data['J'] == '1') & \n",
    "                              (data['PKIS'] == '01') &\n",
    "                              (data['S10'] == '2') & \n",
    "                              ~(data['U'] >= '50')].copy()\n",
    "\n",
    "    # Create a new column 'validation_03' and initialize it with 1 (pass)\n",
    "    data['KONSISTENSI_TC14'] = 1\n",
    "\n",
    "    # Mark the rows that fail the condition as 0 (fail)\n",
    "    data.loc[filtered_data_TC14.index, 'KONSISTENSI_TC14'] = 0\n",
    "\n",
    "    return data\n",
    "\n",
    "result = validate_condition_TC14(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c5ae79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_condition_TC15a(data):\n",
    "    # Apply the condition filters\n",
    "    filtered_data_TC15a = data[(data['TP'] == 1) & \n",
    "                              (data['PKIS'].isin(['02', '04', '05', '07', '08']))].copy()\n",
    "\n",
    "    # Create a new column 'validation_03' and initialize it with 1 (pass)\n",
    "    data['KONSISTENSI_TC15a'] = 1\n",
    "\n",
    "    # Mark the rows that fail the condition as 0 (fail)\n",
    "    data.loc[filtered_data_TC15a.index, 'KONSISTENSI_TC15a'] = 0\n",
    "\n",
    "    return data\n",
    "\n",
    "result = validate_condition_TC15a(df)\n",
    "\n",
    "################################################################################\n",
    "\n",
    "def validate_condition_TC15b(data):\n",
    "    # Apply the condition filters\n",
    "    filtered_data_TC15b = data[(data['TP'].isin([2, 3, 4, 5])) & \n",
    "                              (data['U'] <= '17')].copy()\n",
    "\n",
    "    # Create a new column 'validation_03' and initialize it with 1 (pass)\n",
    "    data['KONSISTENSI_TC15b'] = 1\n",
    "\n",
    "    # Mark the rows that fail the condition as 0 (fail)\n",
    "    data.loc[filtered_data_TC15b.index, 'KONSISTENSI_TC15b'] = 0\n",
    "\n",
    "    return data\n",
    "\n",
    "result = validate_condition_TC15b(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e88a574",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_condition_TC16a(data):\n",
    "    # Apply the condition filters\n",
    "    filtered_data_TC16a = data[(data['P'] == 1) &\n",
    "                               (data['S10'].isin([1, 7])) &\n",
    "                               (data['S15'].isin([1, 2, 3]))].copy()\n",
    "\n",
    "    # Create a new column 'validation_03' and initialize it with 1 (pass)\n",
    "    data['KONSISTENSI_TC16a'] = 1\n",
    "\n",
    "    # Mark the rows that fail the condition as 0 (fail)\n",
    "    data.loc[filtered_data_TC16a.index, 'KONSISTENSI_TC16a'] = 0\n",
    "\n",
    "    return data\n",
    "\n",
    "result = validate_condition_TC16a(df)\n",
    "\n",
    "################################################################################\n",
    "\n",
    "def validate_condition_TC16b(data):\n",
    "    # Apply the condition filters\n",
    "    filtered_data_TC16b = data[(data['P'] == 2) & (data['S10'].isin([3, 8, 9, 12, 13]))].copy()\n",
    "\n",
    "    # Create a new column 'validation_03' and initialize it with 1 (pass)\n",
    "    data['KONSISTENSI_TC16b'] = 1\n",
    "\n",
    "    # Mark the rows that fail the condition as 0 (fail)\n",
    "    data.loc[filtered_data_TC16b.index, 'KONSISTENSI_TC16b'] = 0\n",
    "\n",
    "    return data\n",
    "\n",
    "result = validate_condition_TC16b(df)\n",
    "                               \n",
    "################################################################################\n",
    "\n",
    "def validate_condition_TC16c(data):\n",
    "    # Apply the condition filters\n",
    "    filtered_data_TC16c = data[(data['P'] == 4) & (data['S10'].isin([1, 7]))].copy()\n",
    "\n",
    "    # Create a new column 'validation_03' and initialize it with 1 (pass)\n",
    "    data['KONSISTENSI_TC16c'] = 1\n",
    "\n",
    "    # Mark the rows that fail the condition as 0 (fail)\n",
    "    data.loc[filtered_data_TC16c.index, 'KONSISTENSI_TC16c'] = 0\n",
    "\n",
    "    return data\n",
    "\n",
    "result = validate_condition_TC16c(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2c9b0f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df['S8'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c5786b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['KONSISTENSI_TC16c'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29f7249",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def print_column_value_counts(df):\n",
    "    for column in df.columns:\n",
    "        if column.startswith('KONSISTENSI'):\n",
    "            value_counts = df[column].value_counts()\n",
    "            total_count = value_counts.sum()\n",
    "            print(f\"Column: {column}\")\n",
    "            for value, count in value_counts.items():\n",
    "                percentage = (count / total_count) * 100\n",
    "                result = value  # Display the value as is\n",
    "                print(f\"Result: {result}, Count: {count}, Percentage: {percentage:.2f}%\")\n",
    "            print()  # Add an empty line between columns\n",
    "\n",
    "print_column_value_counts(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f015a3ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbf47d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed7fe98",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['RIN'].unique())\n",
    "print(df['RIN'].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac93521f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_01 = df[df['RIN'] == '1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5ec975",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_01['RIN'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f30c97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['NOIC'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11eaa54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['NOIC'].isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c9d3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data(df):\n",
    "    # Filter the data to get only rows that contain 'RIN' == 1\n",
    "    df = df[df['RIN'] == '1'].copy()\n",
    "\n",
    "    # Iterate over every row and check the conditions\n",
    "    results = []\n",
    "    for _, row in df.iterrows():\n",
    "        s8_result = row['S8'] in [1000, 200, 30, 4, 1004, 204, 34] if not np.isnan(row['S8']) else False\n",
    "        s18_result = len(str(row['S18'])) == 6\n",
    "        s19_result = len(str(row['S19'])) == 5\n",
    "        s20_result = row['S20'] in range(1, 7) if not np.isnan(row['S20']) else False\n",
    "\n",
    "        result = (\n",
    "            (row['S1'] == 1) and\n",
    "            (len(str(int(row['S3']))) == 2 and row['S3'] >= 30) and\n",
    "            (row['S7'] in [1, 2]) and\n",
    "            s8_result and\n",
    "            s18_result and\n",
    "            s19_result and\n",
    "            s20_result and\n",
    "            (row['S21'] in range(1, 4)) and\n",
    "            (row['S22'] in range(1, 4)) and\n",
    "            (row['S23'] in range(1, 12)) and\n",
    "            (row['S24'] in [1, 2]) and\n",
    "            (row['S34'] in [1, 2])\n",
    "        )\n",
    "        results.append(int(result))\n",
    "\n",
    "    # Add the results as a new column\n",
    "    df['combined_result'] = results\n",
    "\n",
    "    return df\n",
    "\n",
    "df = check_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19275e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['combined_result'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781065db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rin_1 = df[df['combined_result'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fb6624",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rin_1[['S1', 'S3', 'S7', 'S8', 'S18', 'S19', 'S20', 'S21', 'S22', 'S23', 'S24', 'S34', 'combined_result']].sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c05e40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fccd5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae97ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the original file name\n",
    "original_file_name = os.path.basename(dataset_path)\n",
    "\n",
    "suffix = '_konsistensi'\n",
    "\n",
    "# Create the new file name by adding the suffix and changing the extension to .xlsx\n",
    "new_file_name = original_file_name.replace('.xlsx', '') + suffix + '.xlsx'\n",
    "\n",
    "# Save the DataFrame to Excel using the new file name\n",
    "df.to_excel(os.path.join(output_file_path, new_file_name), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f015cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the original file name\n",
    "original_file_name = os.path.basename(dataset_path)\n",
    "\n",
    "suffix = '_konsistensi'\n",
    "\n",
    "# Create the new file name by adding the suffix\n",
    "new_file_name = original_file_name.replace('.csv', '') + suffix + '.csv'\n",
    "\n",
    "# Save the DataFrame as CSV using the new file name\n",
    "df.to_csv(os.path.join(output_file_path, new_file_name), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdebcce1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c75ad8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d973fb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
